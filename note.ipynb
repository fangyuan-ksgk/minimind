{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c4ac37",
   "metadata": {},
   "source": [
    "#### Self-organizing LLM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414612cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SORL Generation Results ---\n",
      "Base vocabulary size: 11\n",
      "Total vocabulary size: 62\n",
      "\n",
      "Generated Sequence: tensor([[41, 17, 41, 41, 17, 41, 41, 17, 41, 17, 53,  1,  1,  1, 53,  1,  1]])\n",
      "\n",
      "--- Forward propagation (sparse attention) ---\n",
      "result.logits.shape:  torch.Size([1, 3, 62])\n",
      "\n",
      "--- Denoising ---\n",
      "Generating 2 level-1 tokens in parallel: [1, 2, 3, 61, 2, 4, 1, 61, 3, 4, 2, 61] --> [1, 2, 3, 60, 2, 4, 1, 16, 3, 4, 2, 17]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_sorl import infer_level\n",
    "from model.model_minimind import MiniMindConfig\n",
    "\n",
    "full_vocab_list = [11, 50] # Base vocab + abstract vocabs\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=MiniMindConfig(vocab_size=sum(full_vocab_list)), # Config needs the total new vocab size\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=5,\n",
    "    pad_token_id=0\n",
    ")\n",
    "# --- Generate text using the custom SORL logic ---\n",
    "\n",
    "prompt = torch.tensor([[1, 2, 3]])\n",
    "generated_sequence = model.generate(\n",
    "    input_ids=prompt,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0,\n",
    "    top_k=50,\n",
    "    force_abstraction_every_n=4  # Example: force an abstraction token every 10 steps\n",
    ")\n",
    "\n",
    "print(\"--- SORL Generation Results ---\")\n",
    "print(\"Base vocabulary size:\", model.vocab_sizes[0].item())\n",
    "print(\"Total vocabulary size:\", model.model.config.vocab_size)\n",
    "print(\"\\nGenerated Sequence:\", generated_sequence)\n",
    "\n",
    "\n",
    "result = model.forward(prompt)\n",
    "print(\"\\n--- Forward propagation (sparse attention) ---\")\n",
    "print(\"result.logits.shape: \", result.logits.shape)\n",
    "\n",
    "\n",
    "orig_tokens = torch.tensor([[1,2,3,61,2,4,1,61,3,4,2,61]])\n",
    "\n",
    "levels = infer_level(orig_tokens, model.vocab_sizes, -1)\n",
    "denoise_mask = torch.isin(orig_tokens, model.level_mask_tokens[1:])\n",
    "denoise_levels = levels[denoise_mask]\n",
    "\n",
    "new_tokens = model.denoise(orig_tokens, denoise_mask, denoise_levels, 0.0)\n",
    "print(\"\\n--- Denoising ---\")\n",
    "print(f\"Generating 2 level-1 tokens in parallel: {orig_tokens[0].tolist()} --> {new_tokens[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737569fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "--- Initializing Model and SORL Configuration ---\n",
      "================================================================================\n",
      "Model Initialized. Total vocabulary size: 642\n",
      "SORL Config: SORLConfig(n=4, temperature=1.0, K=8, causal_rollout=False, budget=None, l=1, steps=4, max_t_search=32, start_ts=None, end_ts=None, abstract_budget=5, use_rhythmic_placeholders=True, use_spike_placeholders=False, curriculum_ratio=0.6, max_seq_len=None, use_fade_memory=False, min_keep=1024, train_dataset_path=None, val_dataset_path=None, train_batch_size=128, val_batch_size=128, train_iterations=1000, val_iterations=10, max_length=1024, learning_rate=0.001, log_interval=100)\n",
      "\n",
      "Created dummy data with shape: torch.Size([2, 128])\n",
      "\n",
      "================================================================================\n",
      "--- Testing `sorl_search` ---\n",
      "================================================================================\n",
      "Original sequence length: 128\n",
      "Sequence length after search (with abstractions): 143\n",
      "Abstraction switch ratio: 1.00\n",
      "✅ `sorl_search` test passed.\n",
      "\n",
      "================================================================================\n",
      "--- Testing Loss Computation ---\n",
      "================================================================================\n",
      "Per-token loss shape: torch.Size([2, 142])\n",
      "Trajectory Loss (ssl_loss): 6.6709\n",
      "Abstraction Loss (abs_loss): 6.3879\n",
      "Total Loss: 13.0588\n",
      "✅ Loss computation test passed.\n",
      "\n",
      "================================================================================\n",
      "--- Testing `evaluate` ---\n",
      "================================================================================\n",
      "Greedy trajectory perplexity: 6.6612\n",
      "Search improvement over greedy: 0.02%\n",
      "✅ `evaluate` test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_minimind import MiniMindConfig\n",
    "# from model.model_sorl import \n",
    "from src.sorl import SORLConfig, sorl_search, compute_per_token_loss, compute_loss, evaluate\n",
    "\n",
    "# --- 1. Setup the Model and Configuration ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Initializing Model and SORL Configuration ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize a SORL-wrapped MiniMind model from scratch for the test\n",
    "base_vocab_size = 512\n",
    "abstract_vocab_sizes = [128]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=64, # Using smaller dimensions for faster testing\n",
    "    num_attention_heads=2,\n",
    "    num_hidden_layers=2,\n",
    "    intermediate_size=128,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "sorl_model = SorlModelWrapper(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=1024\n",
    ")\n",
    "\n",
    "# Create a configuration for the SORL search algorithm\n",
    "# These parameters control how abstraction is performed\n",
    "sorl_config = SORLConfig(\n",
    "    n=4,                    # Number of candidates to roll out\n",
    "    temperature=1.0,        # Temperature for sampling abstract tokens\n",
    "    K=8,                    # Rhythmic stride for level-1 abstraction\n",
    "    l=1,                    # The abstraction level to search for\n",
    "    steps=4,                # Steps for chunk-wise denoising\n",
    "    max_t_search=32,        # Max number of abstract timestamps to search within\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False # Disable spike for simplicity in this test\n",
    ")\n",
    "\n",
    "print(f\"Model Initialized. Total vocabulary size: {sorl_model.model.config.vocab_size}\")\n",
    "print(f\"SORL Config: {sorl_config}\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Create Dummy Data ---\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "# Create a batch of random token sequences\n",
    "dummy_data = torch.randint(0, base_vocab_size, (batch_size, seq_len), device=sorl_model.model.device)\n",
    "print(f\"Created dummy data with shape: {dummy_data.shape}\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Test the `sorl_search` Function ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing `sorl_search` ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# This is the core of the SORL algorithm. It takes the original data and\n",
    "# finds a better representation by inserting abstract tokens.\n",
    "with torch.no_grad():\n",
    "    best_sequence, switch_ratio = sorl_search(dummy_data, sorl_model, sorl_config)\n",
    "\n",
    "print(f\"Original sequence length: {dummy_data.shape[1]}\")\n",
    "print(f\"Sequence length after search (with abstractions): {best_sequence.shape[1]}\")\n",
    "print(f\"Abstraction switch ratio: {switch_ratio:.2f}\")\n",
    "# The switch ratio indicates how often the algorithm preferred a sampled abstraction over the greedy one.\n",
    "assert best_sequence.shape[0] == batch_size\n",
    "assert best_sequence.shape[1] > seq_len # Should be longer due to added placeholders\n",
    "print(\"✅ `sorl_search` test passed.\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Test Loss Computation ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing Loss Computation ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute the loss on the improved sequence found by the search.\n",
    "# This is what would be used for the backward pass during training.\n",
    "ppt = compute_per_token_loss(sorl_model, best_sequence)\n",
    "ssl_loss, abs_loss = compute_loss(best_sequence, sorl_model, ppt)\n",
    "total_loss = ssl_loss + abs_loss\n",
    "\n",
    "print(f\"Per-token loss shape: {ppt.shape}\")\n",
    "print(f\"Trajectory Loss (ssl_loss): {ssl_loss.item():.4f}\")\n",
    "print(f\"Abstraction Loss (abs_loss): {abs_loss.item():.4f}\")\n",
    "print(f\"Total Loss: {total_loss.item():.4f}\")\n",
    "assert ssl_loss.ndim == 0 and abs_loss.ndim == 0\n",
    "print(\"✅ Loss computation test passed.\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Test the `evaluate` Function ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing `evaluate` ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# This function simulates a validation step, comparing a greedy search\n",
    "# against a random search to measure the potential for improvement.\n",
    "with torch.no_grad():\n",
    "    greedy_ppl, improve_ppl_percent, _, _ = evaluate(dummy_data, sorl_model, n=4, config=sorl_config)\n",
    "\n",
    "print(f\"Greedy trajectory perplexity: {greedy_ppl.item():.4f}\")\n",
    "print(f\"Search improvement over greedy: {improve_ppl_percent.item():.2f}%\")\n",
    "print(\"✅ `evaluate` test passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2214dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "--- Initializing Model, Tokenizer, and SORL Configuration ---\n",
      "================================================================================\n",
      "Tokenizer loaded. Vocab size: 6400\n",
      "Model Initialized. Total vocabulary size: 6530\n",
      "\n",
      "================================================================================\n",
      "--- Testing `get_data_loader` with real data ---\n",
      "================================================================================\n",
      "Successfully loaded one batch of data.\n",
      "Batch shape (X): torch.Size([2, 255])\n",
      "Data type: torch.int64\n",
      "✅ `get_data_loader` test passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load language modeling data inside & train on them ~ \n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from src.sorl import SORLConfig, sorl_search, compute_per_token_loss, compute_loss\n",
    "from dataset.utils import get_data_loader\n",
    "\n",
    "# --- 1. Setup Model, Tokenizer, and Configuration ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Initializing Model, Tokenizer, and SORL Configuration ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the tokenizer that will be used for the dataset.\n",
    "# This should match the tokenizer your model was trained with.\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('model/')\n",
    "    print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load tokenizer from 'model/'. Make sure tokenizer files are present. Error: {e}\")\n",
    "    # Fallback to a default tokenizer if needed for the test to run\n",
    "    # tokenizer = AutoTokenizer.from_pretrained('gpt2') \n",
    "    # print(\"Using gpt2 tokenizer as a fallback.\")\n",
    "\n",
    "# Initialize a SORL-wrapped MiniMind model for the test.\n",
    "# The base vocab size should match the tokenizer's.\n",
    "base_vocab_size = tokenizer.vocab_size\n",
    "abstract_vocab_sizes = [128]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=64, # Using smaller dimensions for faster testing\n",
    "    num_attention_heads=2,\n",
    "    num_hidden_layers=2,\n",
    "    intermediate_size=128,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "sorl_model = SorlModelWrapper(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=1024\n",
    ")\n",
    "print(f\"Model Initialized. Total vocabulary size: {sorl_model.model.config.vocab_size}\\n\")\n",
    "\n",
    "# --- 2. Test the Data Loader ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing `get_data_loader` with real data ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a configuration for the SORL search algorithm, now including dataset paths\n",
    "sorl_config = SORLConfig(\n",
    "    n=4,\n",
    "    temperature=1.0,\n",
    "    K=8,\n",
    "    l=1,\n",
    "    steps=4,\n",
    "    max_t_search=32,\n",
    "    train_dataset_path='dataset/pretrain_hq.jsonl',\n",
    "    train_batch_size=2,\n",
    "    max_length=256, # Using a smaller sequence length for faster testing\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False\n",
    ")\n",
    "\n",
    "# Create the data loader using the new utility function\n",
    "\n",
    "train_loader = get_data_loader(\n",
    "    dataset_path=sorl_config.train_dataset_path,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=sorl_config.train_batch_size,\n",
    "    max_length=sorl_config.max_length\n",
    ")\n",
    "\n",
    "# Fetch one batch of data\n",
    "X, Y, loss_mask = next(iter(train_loader))\n",
    "\n",
    "print(f\"Successfully loaded one batch of data.\")\n",
    "print(f\"Batch shape (X): {X.shape}\")\n",
    "print(f\"Data type: {X.dtype}\")\n",
    "print(\"✅ `get_data_loader` test passed.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ae6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.base import MemLoader\n",
    "\n",
    "dataset = MemLoader(filepath='dataset/pretrain_hq.bin', device=\"cpu\")\n",
    "\n",
    "# dataset.get_batch(batch_size=10) # now this is immediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbcd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, mask = dataset.get_batch(batch_size=10) # now this is immediate\n",
    "\n",
    "# train with SoRL missing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cae666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing training components ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemLoader initialized.\n",
      "SORL Model initialized.\n",
      "--- Initialization Complete ---\n",
      "\n",
      "--- Running one SORL training step ---\n",
      "Fetched data batch of shape: torch.Size([4, 255])\n",
      "SORL search complete. New sequence shape: torch.Size([4, 286])\n",
      "Computed Loss -> Total: 17.7018 (SSL: 8.9489, Abs: 8.7529)\n",
      "Optimizer step complete (weights have been updated).\n",
      "\n",
      "--- ✅ Single training step finished! ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from dataset.base import MemLoader\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from src.sorl import SORLConfig, sorl_search, compute_per_token_loss, compute_loss\n",
    "\n",
    "# --- 1. Full Pipeline Initialization ---\n",
    "print(\"--- Initializing training components ---\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer to get vocab size and pad token id\n",
    "tokenizer = AutoTokenizer.from_pretrained('model/')\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Initialize the high-performance memory-mapped data loader\n",
    "dataset = MemLoader('dataset/pretrain_hq.bin', device=device)\n",
    "print(\"MemLoader initialized.\")\n",
    "\n",
    "# Initialize the SORL-wrapped model\n",
    "base_vocab_size = tokenizer.vocab_size\n",
    "abstract_vocab_sizes = [128]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=256, num_attention_heads=4, num_hidden_layers=4,\n",
    "    intermediate_size=512, vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "# The .to(device) call will now work correctly\n",
    "sorl_model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=1024\n",
    ").to(device)\n",
    "print(\"SORL Model initialized.\")\n",
    "\n",
    "# Configure the SORL search algorithm\n",
    "sorl_config = SORLConfig(\n",
    "    n=4, temperature=1.0, K=8, l=1, steps=4, max_t_search=32,\n",
    "    use_rhythmic_placeholders=True, use_spike_placeholders=False\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(sorl_model.model.parameters(), lr=1e-4)\n",
    "print(\"--- Initialization Complete ---\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Perform a Single SORL Training Step ---\n",
    "print(\"--- Running one SORL training step ---\")\n",
    "# Get a batch of data instantly\n",
    "data_batch, _ = dataset.get_batch(batch_size=4)\n",
    "print(f\"Fetched data batch of shape: {data_batch.shape}\")\n",
    "\n",
    "# a) SORL Search Step (run in no_grad context)\n",
    "with torch.no_grad():\n",
    "    search_data, switch_ratio = sorl_search(data_batch, sorl_model, sorl_config)\n",
    "print(f\"SORL search complete. New sequence shape: {search_data.shape}\")\n",
    "\n",
    "# b) Forward Pass: Compute per-token loss on the \"improved\" data\n",
    "ppt = compute_per_token_loss(sorl_model, search_data)\n",
    "\n",
    "# c) Compute final SORL loss (combining trajectory and abstraction losses)\n",
    "ssl_loss, abs_loss = compute_loss(search_data, sorl_model, ppt)\n",
    "total_loss = ssl_loss + abs_loss\n",
    "print(f\"Computed Loss -> Total: {total_loss.item():.4f} (SSL: {ssl_loss.item():.4f}, Abs: {abs_loss.item():.4f})\")\n",
    "\n",
    "# d) Backward Pass and Optimizer Step\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Optimizer step complete (weights have been updated).\")\n",
    "\n",
    "print(\"\\n--- ✅ Single training step finished! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e06c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
