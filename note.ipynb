{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c4ac37",
   "metadata": {},
   "source": [
    "#### Self-organizing LLM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414612cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SORL Generation Results ---\n",
      "Base vocabulary size: 11\n",
      "Total vocabulary size: 62\n",
      "\n",
      "Generated Sequence: tensor([[41, 17, 41, 41, 17, 41, 41, 17, 41, 17, 53,  1,  1,  1, 53,  1,  1]])\n",
      "\n",
      "--- Forward propagation (sparse attention) ---\n",
      "result.logits.shape:  torch.Size([1, 3, 62])\n",
      "\n",
      "--- Denoising ---\n",
      "Generating 2 level-1 tokens in parallel: [1, 2, 3, 61, 2, 4, 1, 61, 3, 4, 2, 61] --> [1, 2, 3, 60, 2, 4, 1, 16, 3, 4, 2, 17]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_sorl import infer_level\n",
    "from model.model_minimind import MiniMindConfig\n",
    "\n",
    "full_vocab_list = [11, 50] # Base vocab + abstract vocabs\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=MiniMindConfig(vocab_size=sum(full_vocab_list)), # Config needs the total new vocab size\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=5,\n",
    "    pad_token_id=0\n",
    ")\n",
    "# --- Generate text using the custom SORL logic ---\n",
    "\n",
    "prompt = torch.tensor([[1, 2, 3]])\n",
    "generated_sequence = model.generate(\n",
    "    input_ids=prompt,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0,\n",
    "    top_k=50,\n",
    "    force_abstraction_every_n=4  # Example: force an abstraction token every 10 steps\n",
    ")\n",
    "\n",
    "print(\"--- SORL Generation Results ---\")\n",
    "print(\"Base vocabulary size:\", model.vocab_sizes[0].item())\n",
    "print(\"Total vocabulary size:\", model.model.config.vocab_size)\n",
    "print(\"\\nGenerated Sequence:\", generated_sequence)\n",
    "\n",
    "\n",
    "result = model.forward(prompt)\n",
    "print(\"\\n--- Forward propagation (sparse attention) ---\")\n",
    "print(\"result.logits.shape: \", result.logits.shape)\n",
    "\n",
    "\n",
    "orig_tokens = torch.tensor([[1,2,3,61,2,4,1,61,3,4,2,61]])\n",
    "\n",
    "levels = infer_level(orig_tokens, model.vocab_sizes, -1)\n",
    "denoise_mask = torch.isin(orig_tokens, model.level_mask_tokens[1:])\n",
    "denoise_levels = levels[denoise_mask]\n",
    "\n",
    "new_tokens = model.denoise(orig_tokens, denoise_mask, denoise_levels, 0.0)\n",
    "print(\"\\n--- Denoising ---\")\n",
    "print(f\"Generating 2 level-1 tokens in parallel: {orig_tokens[0].tolist()} --> {new_tokens[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c314adc",
   "metadata": {},
   "source": [
    "#### Self-organizing Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32cae666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing training components ---\n",
      "MemLoader initialized.\n",
      "SORL Model initialized.\n",
      "--- Initialization Complete ---\n",
      "\n",
      "--- Running one SORL training step ---\n",
      "Fetched data batch of shape: torch.Size([4, 255])\n",
      "SORL search complete. New sequence shape: torch.Size([4, 286])\n",
      "Computed Loss -> Total: 17.5724 (SSL: 8.9874, Abs: 8.5850)\n",
      "Optimizer step complete (weights have been updated).\n",
      "\n",
      "--- ✅ Single training step finished! ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from dataset.base import MemLoader\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from src.sorl import SORLConfig, sorl_search, compute_per_token_loss, compute_loss\n",
    "\n",
    "# --- 1. Full Pipeline Initialization ---\n",
    "print(\"--- Initializing training components ---\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer to get vocab size and pad token id\n",
    "tokenizer = AutoTokenizer.from_pretrained('model/')\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Initialize the high-performance memory-mapped data loader\n",
    "dataset = MemLoader('dataset/pretrain_hq.bin', device=device)\n",
    "print(\"MemLoader initialized.\")\n",
    "\n",
    "# Initialize the SORL-wrapped model\n",
    "base_vocab_size = tokenizer.vocab_size\n",
    "abstract_vocab_sizes = [128]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=256, num_attention_heads=4, num_hidden_layers=4,\n",
    "    intermediate_size=512, vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "# The .to(device) call will now work correctly\n",
    "sorl_model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=1024,\n",
    "    pad_token_id=0\n",
    ").to(device)\n",
    "print(\"SORL Model initialized.\")\n",
    "\n",
    "# Configure the SORL search algorithm\n",
    "sorl_config = SORLConfig(\n",
    "    n=4, temperature=1.0, K=8, l=1, steps=4, max_t_search=32,\n",
    "    use_rhythmic_placeholders=True, use_spike_placeholders=False\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(sorl_model.model.parameters(), lr=1e-4)\n",
    "print(\"--- Initialization Complete ---\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Perform a Single SORL Training Step ---\n",
    "print(\"--- Running one SORL training step ---\")\n",
    "# Get a batch of data instantly\n",
    "data_batch, _ = dataset.get_batch(batch_size=4)\n",
    "print(f\"Fetched data batch of shape: {data_batch.shape}\")\n",
    "\n",
    "# a) SORL Search Step (run in no_grad context)\n",
    "with torch.no_grad():\n",
    "    search_data, switch_ratio = sorl_search(data_batch, sorl_model, sorl_config)\n",
    "print(f\"SORL search complete. New sequence shape: {search_data.shape}\")\n",
    "\n",
    "# b) Forward Pass: Compute per-token loss on the \"improved\" data\n",
    "ppt = compute_per_token_loss(sorl_model, search_data)\n",
    "\n",
    "# c) Compute final SORL loss (combining trajectory and abstraction losses)\n",
    "ssl_loss, abs_loss = compute_loss(search_data, sorl_model, ppt)\n",
    "total_loss = ssl_loss + abs_loss\n",
    "print(f\"Computed Loss -> Total: {total_loss.item():.4f} (SSL: {ssl_loss.item():.4f}, Abs: {abs_loss.item():.4f})\")\n",
    "\n",
    "# d) Backward Pass and Optimizer Step\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Optimizer step complete (weights have been updated).\")\n",
    "\n",
    "print(\"\\n--- ✅ Single training step finished! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dda40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75ee76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
