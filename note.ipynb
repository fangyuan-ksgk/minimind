{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c4ac37",
   "metadata": {},
   "source": [
    "#### Self-organizing LLM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414612cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SORL Generation Results ---\n",
      "Base vocabulary size: 11\n",
      "Total vocabulary size: 62\n",
      "\n",
      "Generated Sequence: tensor([[20, 20, 56, 11, 11, 11, 37, 37, 37, 37, 37,  6,  6,  6, 37,  6,  6]])\n",
      "\n",
      "--- Forward propagation (sparse attention) ---\n",
      "result.logits.shape:  torch.Size([1, 3, 62])\n",
      "\n",
      "--- Denoising ---\n",
      "Generating 2 level-1 tokens in parallel: [1, 2, 3, 61, 2, 4, 1, 61, 3, 4, 2, 61] --> [1, 2, 3, 20, 2, 4, 1, 20, 3, 4, 2, 48]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_sorl import infer_level\n",
    "from model.model_minimind import MiniMindConfig\n",
    "\n",
    "full_vocab_list = [11, 50] # Base vocab + abstract vocabs\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=MiniMindConfig(vocab_size=sum(full_vocab_list)), # Config needs the total new vocab size\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=5,\n",
    "    pad_token_id=0\n",
    ")\n",
    "# --- Generate text using the custom SORL logic ---\n",
    "\n",
    "prompt = torch.tensor([[1, 2, 3]])\n",
    "generated_sequence = model.generate(\n",
    "    input_ids=prompt,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0,\n",
    "    top_k=50,\n",
    "    force_abstraction_every_n=4  # Example: force an abstraction token every 10 steps\n",
    ")\n",
    "\n",
    "print(\"--- SORL Generation Results ---\")\n",
    "print(\"Base vocabulary size:\", model.vocab_sizes[0].item())\n",
    "print(\"Total vocabulary size:\", model.model.config.vocab_size)\n",
    "print(\"\\nGenerated Sequence:\", generated_sequence)\n",
    "\n",
    "\n",
    "result = model.forward(prompt)\n",
    "print(\"\\n--- Forward propagation (sparse attention) ---\")\n",
    "print(\"result.logits.shape: \", result.logits.shape)\n",
    "\n",
    "\n",
    "orig_tokens = torch.tensor([[1,2,3,61,2,4,1,61,3,4,2,61]])\n",
    "\n",
    "levels = infer_level(orig_tokens, model.vocab_sizes, -1)\n",
    "denoise_mask = torch.isin(orig_tokens, model.level_mask_tokens[1:])\n",
    "denoise_levels = levels[denoise_mask]\n",
    "\n",
    "new_tokens = model.denoise(orig_tokens, denoise_mask, denoise_levels, 0.0)\n",
    "print(\"\\n--- Denoising ---\")\n",
    "print(f\"Generating 2 level-1 tokens in parallel: {orig_tokens[0].tolist()} --> {new_tokens[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change on memory fading gadget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd582e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c314adc",
   "metadata": {},
   "source": [
    "#### Self-organizing Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cae666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing training components ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemLoader initialized.\n",
      "SORL Model initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from dataset.base import MemLoader\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from src.sorl import SORLConfig, sorl_search, compute_per_token_loss, compute_loss\n",
    "\n",
    "# --- 1. Full Pipeline Initialization ---\n",
    "print(\"--- Initializing training components ---\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer to get vocab size and pad token id\n",
    "tokenizer = AutoTokenizer.from_pretrained('model/')\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Initialize the high-performance memory-mapped data loader\n",
    "dataset = MemLoader('dataset/pretrain_hq.bin', device=device)\n",
    "print(\"MemLoader initialized.\")\n",
    "\n",
    "# Initialize the SORL-wrapped model\n",
    "base_vocab_size = tokenizer.vocab_size\n",
    "abstract_vocab_sizes = [8]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=256, num_attention_heads=4, num_hidden_layers=4,\n",
    "    intermediate_size=512, vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "# The .to(device) call will now work correctly\n",
    "sorl_model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=1024,\n",
    "    pad_token_id=0\n",
    ").to(device)\n",
    "print(\"SORL Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed5b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change can be made on the attention-masking mechanism\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# 1. A visualization on where abstraction is added, and what is masked out \n",
    "#    I believe when abstraction is added, we only perform a 'distant memory masking'\n",
    "#    I suspect a meomry distillation based training requires masking out the local chunk where abstraction is present\n",
    "\n",
    "# .forward method accepts an 'attention_mask' argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6dda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initialization Complete ---\n",
      "\n",
      "--- Running one SORL training step ---\n",
      "Fetched data batch of shape: torch.Size([4, 255])\n",
      "SORL search complete. New sequence shape: torch.Size([4, 286])\n",
      "Computed Loss -> Total: 17.7112 (SSL: 8.9598, Abs: 8.7514)\n",
      "Optimizer step complete (weights have been updated).\n",
      "\n",
      "--- ✅ Single training step finished! ---\n"
     ]
    }
   ],
   "source": [
    "# Configure the SORL search algorithm\n",
    "sorl_config = SORLConfig(\n",
    "    n=4, temperature=1.0, K=8, l=1, steps=4, max_t_search=32,\n",
    "    use_rhythmic_placeholders=True, use_spike_placeholders=False\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(sorl_model.model.parameters(), lr=1e-4)\n",
    "print(\"--- Initialization Complete ---\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Perform a Single SORL Training Step ---\n",
    "print(\"--- Running one SORL training step ---\")\n",
    "# Get a batch of data instantly\n",
    "data_batch, _ = dataset.get_batch(batch_size=4)\n",
    "print(f\"Fetched data batch of shape: {data_batch.shape}\")\n",
    "\n",
    "# a) SORL Search Step (run in no_grad context)\n",
    "with torch.no_grad():\n",
    "    search_data, switch_ratio = sorl_search(data_batch, sorl_model, sorl_config)\n",
    "print(f\"SORL search complete. New sequence shape: {search_data.shape}\")\n",
    "\n",
    "# b) Forward Pass: Compute per-token loss on the \"improved\" data\n",
    "ppt = compute_per_token_loss(sorl_model, search_data)\n",
    "\n",
    "# c) Compute final SORL loss (combining trajectory and abstraction losses)\n",
    "ssl_loss, abs_loss = compute_loss(search_data, sorl_model, ppt)\n",
    "total_loss = ssl_loss + abs_loss\n",
    "print(f\"Computed Loss -> Total: {total_loss.item():.4f} (SSL: {ssl_loss.item():.4f}, Abs: {abs_loss.item():.4f})\")\n",
    "\n",
    "# d) Backward Pass and Optimizer Step\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Optimizer step complete (weights have been updated).\")\n",
    "\n",
    "print(\"\\n--- ✅ Single training step finished! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0c114",
   "metadata": {},
   "source": [
    "#### Subway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52c7aa",
   "metadata": {},
   "source": [
    "#### Hidden Information compressed into Abstraction\n",
    "- Question #1. Do we need the loss mask here? \n",
    "- Answer #1. Yes, because we want to compare whether abstraction can replace memories, the effect should be on the prediction perplexity only to remove other factors, such as perplexity of provided number etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59133bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating samples: 100%|██████████| 100/100 [00:00<00:00, 209610.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare 'hidden information' dataset \n",
    "# xxxA: xxx is its basic form \n",
    "# we apply memory compression & SoRL to ask the model to predict \n",
    "# A: xxx\n",
    "# Here, xxx is hidden information, can be a number etc. \n",
    "# ----------------------------------------------------------------\n",
    "# This dataset will be useful to inspect whether abstraction is effective or not. \n",
    "# ----------------------------------------------------------------\n",
    "# python -m dataset.prep_hidden_info_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320175b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n",
      "Model initialized on cpu with 25.90M parameters.\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer\n",
    "import sys, torch\n",
    "\n",
    "# --- Add project root to path ---\n",
    "# You might need to adjust this depending on your notebook's location\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from dataset.base import MemLoader\n",
    "from src.sorl import SORLConfig, sorl_search, compute_loss, compute_per_token_loss, GatedPhaseTransition, SearchScheduler\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration (Mimicking command-line args)\n",
    "# ==============================================================================\n",
    "args = SimpleNamespace(\n",
    "    # --- Paths ---\n",
    "    train_data_path=\"dataset/hidden_info.bin\",\n",
    "    tokenizer_path=\"model/\",\n",
    "    \n",
    "    # --- Model Config ---\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    abstract_vocab_sizes=\"128\",\n",
    "    \n",
    "    # --- Training Config ---\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size=4,\n",
    "    learning_rate=3e-4,\n",
    "    \n",
    "    # --- SORL Config ---\n",
    "    n_rollout=4,\n",
    "    temperature=1.0,\n",
    "    K=8,\n",
    "    denoise_steps=4,\n",
    "    max_t_search=32,\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False,\n",
    "    abstract_budget=5,\n",
    "    temperature_flip=False,\n",
    "    \n",
    "    # --- Curriculum and Memory ---\n",
    "    curriculum_ratio=0.6,\n",
    "    train_iterations=1000, # This will be used by the scheduler\n",
    "    use_fade_memory=False,\n",
    "    use_compression_mask=True, # <-- Set to True to test your new mask\n",
    "    memory_span=256,\n",
    "    \n",
    "    # --- GAPT ---\n",
    "    default_phase=None, # Set to 1 or 2 to override, None to enable GAPT\n",
    "    delta=0.01,\n",
    "    tau=0.1,\n",
    "    p_m=10,\n",
    "    p_c=10\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = tokenizer.vocab_size\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=args.memory_span,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model initialized on {args.device} with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")\n",
    "\n",
    "# --- SORL Config and Schedulers ---\n",
    "sorl_config = SORLConfig(\n",
    "    n=args.n_rollout, temperature=args.temperature, K=args.K,\n",
    "    l=1, steps=args.denoise_steps, max_t_search=args.max_t_search,\n",
    "    use_rhythmic_placeholders=args.use_rhythmic_placeholders,\n",
    "    use_spike_placeholders=args.use_spike_placeholders,\n",
    "    abstract_budget=args.abstract_budget,\n",
    "    temperature_flip=args.temperature_flip,\n",
    "    curriculum_ratio=args.curriculum_ratio,\n",
    "    use_fade_memory=args.use_fade_memory,\n",
    "    use_compression_mask=args.use_compression_mask,\n",
    "    min_keep=args.memory_span, max_seq_len=train_loader.max_length,\n",
    "    train_iterations=args.train_iterations, max_length=train_loader.max_length,\n",
    "    default_phase=args.default_phase, delta=args.delta, tau=args.tau,\n",
    "    p_m=args.p_m, p_c=args.p_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1406805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 8.9716 (SSL: 8.9716, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 02 | Loss: 8.7469 (SSL: 8.7469, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 03 | Loss: 8.5018 (SSL: 8.5018, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.01\n",
      "Step 04 | Loss: 8.3266 (SSL: 8.3266, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.01\n",
      "Step 05 | Loss: 8.4737 (SSL: 8.4737, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.01\n",
      "Step 06 | Loss: 8.1131 (SSL: 8.1131, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.01\n",
      "Step 07 | Loss: 8.0125 (SSL: 8.0125, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.01\n",
      "Step 08 | Loss: 7.8358 (SSL: 7.8358, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.01\n",
      "Step 09 | Loss: 7.8282 (SSL: 7.8282, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.02\n",
      "Step 10 | Loss: 7.9554 (SSL: 7.9554, Abs: 0.0000) | Phase: 1 | t_search: 0 | drop_ratio: 0.02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "search_scheduler = SearchScheduler(sorl_config)\n",
    "gapt = GatedPhaseTransition(sorl_config.delta, sorl_config.tau, sorl_config.p_m, sorl_config.p_c)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "for i in range(10): # Run for 10 steps\n",
    "    # --- Scheduler Step ---\n",
    "    t_search, drop_ratio = search_scheduler.step()\n",
    "    sorl_config.max_t_search = t_search\n",
    "    model.drop_ratio = drop_ratio\n",
    "    \n",
    "    # --- Get data and perform SORL search ---\n",
    "    data, _ = train_loader.get_batch(args.batch_size)\n",
    "    with torch.no_grad():\n",
    "        search_data, switch_ratio = sorl_search(data, model, sorl_config)\n",
    "        \n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, search_data)\n",
    "    ssl_loss, abs_loss = compute_loss(search_data, model, ppt)\n",
    "    \n",
    "    # --- GAPT adaptation ---\n",
    "    current_phase = gapt.step(ssl_loss.item(), abs_loss.item())\n",
    "    if sorl_config.default_phase is not None:\n",
    "        current_phase = sorl_config.default_phase\n",
    "    \n",
    "    total_loss = ssl_loss + abs_loss if current_phase == 2 else ssl_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.4f} (SSL: {ssl_loss.item():.4f}, Abs: {abs_loss.item():.4f}) | \"\n",
    "        f\"Phase: {current_phase} | \"\n",
    "        f\"t_search: {t_search} | \"\n",
    "        f\"drop_ratio: {drop_ratio:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91cda31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.use_compression_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994064d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
