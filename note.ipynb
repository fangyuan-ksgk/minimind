{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c4ac37",
   "metadata": {},
   "source": [
    "#### Self-organizing LLM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414612cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SORL Generation Results ---\n",
      "Base vocabulary size: 12\n",
      "Total vocabulary size: 63\n",
      "\n",
      "Generated Sequence: tensor([[49, 49, 13, 13, 13, 13, 13, 13, 13, 13, 13,  1,  1,  1, 13,  8,  8]])\n",
      "\n",
      "--- Forward propagation (sparse attention) ---\n",
      "result.logits.shape:  torch.Size([1, 3, 63])\n",
      "\n",
      "--- Denoising ---\n",
      "Generating 2 level-1 tokens in parallel: [1, 2, 3, 62, 2, 4, 1, 62, 3, 4, 2, 62] --> [1, 2, 3, 49, 2, 4, 1, 49, 3, 4, 2, 49]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_sorl import infer_level\n",
    "from model.model_minimind import MiniMindConfig\n",
    "\n",
    "full_vocab_list = [11, 50] # Base vocab + abstract vocabs\n",
    "model = SorlModelWrapper(\n",
    "    config=MiniMindConfig(vocab_size=sum(full_vocab_list)), # Config needs the total new vocab size\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=5\n",
    ")\n",
    "# --- Generate text using the custom SORL logic ---\n",
    "\n",
    "prompt = torch.tensor([[1, 2, 3]])\n",
    "generated_sequence = model.generate(\n",
    "    input_ids=prompt,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0,\n",
    "    top_k=50,\n",
    "    force_abstraction_every_n=4  # Example: force an abstraction token every 10 steps\n",
    ")\n",
    "\n",
    "print(\"--- SORL Generation Results ---\")\n",
    "print(\"Base vocabulary size:\", model.vocab_sizes[0].item())\n",
    "print(\"Total vocabulary size:\", model.model.config.vocab_size)\n",
    "print(\"\\nGenerated Sequence:\", generated_sequence)\n",
    "\n",
    "\n",
    "result = model.forward(prompt)\n",
    "print(\"\\n--- Forward propagation (sparse attention) ---\")\n",
    "print(\"result.logits.shape: \", result.logits.shape)\n",
    "\n",
    "\n",
    "orig_tokens = torch.tensor([[1,2,3,62,2,4,1,62,3,4,2,62]])\n",
    "\n",
    "levels = infer_level(orig_tokens, model.vocab_sizes, -1)\n",
    "denoise_mask = torch.isin(orig_tokens, model.level_mask_tokens[1:])\n",
    "denoise_levels = levels[denoise_mask]\n",
    "\n",
    "new_tokens = model.denoise(orig_tokens, denoise_mask, denoise_levels, 0.0)\n",
    "print(\"\\n--- Denoising ---\")\n",
    "print(f\"Generating 2 level-1 tokens in parallel: {orig_tokens[0].tolist()} --> {new_tokens[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737569fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "--- Initializing Model and SORL Configuration ---\n",
      "================================================================================\n",
      "Model Initialized. Total vocabulary size: 642\n",
      "SORL Config: SORLConfig(n=4, temperature=1.0, K=8, causal_rollout=False, budget=None, l=1, steps=4, max_t_search=32, start_ts=None, end_ts=None, abstract_budget=5, use_rhythmic_placeholders=True, use_spike_placeholders=False, curriculum_ratio=0.6, max_seq_len=None, use_fade_memory=False, min_keep=1024, train_dataset_path=None, val_dataset_path=None, train_batch_size=128, val_batch_size=128, train_iterations=1000, val_iterations=10, max_length=1024, learning_rate=0.001, log_interval=100)\n",
      "\n",
      "Created dummy data with shape: torch.Size([2, 128])\n",
      "\n",
      "================================================================================\n",
      "--- Testing `sorl_search` ---\n",
      "================================================================================\n",
      "Original sequence length: 128\n",
      "Sequence length after search (with abstractions): 143\n",
      "Abstraction switch ratio: 1.00\n",
      "✅ `sorl_search` test passed.\n",
      "\n",
      "================================================================================\n",
      "--- Testing Loss Computation ---\n",
      "================================================================================\n",
      "Per-token loss shape: torch.Size([2, 142])\n",
      "Trajectory Loss (ssl_loss): 6.6709\n",
      "Abstraction Loss (abs_loss): 6.3879\n",
      "Total Loss: 13.0588\n",
      "✅ Loss computation test passed.\n",
      "\n",
      "================================================================================\n",
      "--- Testing `evaluate` ---\n",
      "================================================================================\n",
      "Greedy trajectory perplexity: 6.6612\n",
      "Search improvement over greedy: 0.02%\n",
      "✅ `evaluate` test passed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from model.model_minimind import MiniMindConfig\n",
    "# from model.model_sorl import \n",
    "from src.sorl import SORLConfig, sorl_search, compute_per_token_loss, compute_loss, evaluate\n",
    "\n",
    "# --- 1. Setup the Model and Configuration ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Initializing Model and SORL Configuration ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize a SORL-wrapped MiniMind model from scratch for the test\n",
    "base_vocab_size = 512\n",
    "abstract_vocab_sizes = [128]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=64, # Using smaller dimensions for faster testing\n",
    "    num_attention_heads=2,\n",
    "    num_hidden_layers=2,\n",
    "    intermediate_size=128,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "sorl_model = SorlModelWrapper(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=1024\n",
    ")\n",
    "\n",
    "# Create a configuration for the SORL search algorithm\n",
    "# These parameters control how abstraction is performed\n",
    "sorl_config = SORLConfig(\n",
    "    n=4,                    # Number of candidates to roll out\n",
    "    temperature=1.0,        # Temperature for sampling abstract tokens\n",
    "    K=8,                    # Rhythmic stride for level-1 abstraction\n",
    "    l=1,                    # The abstraction level to search for\n",
    "    steps=4,                # Steps for chunk-wise denoising\n",
    "    max_t_search=32,        # Max number of abstract timestamps to search within\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False # Disable spike for simplicity in this test\n",
    ")\n",
    "\n",
    "print(f\"Model Initialized. Total vocabulary size: {sorl_model.model.config.vocab_size}\")\n",
    "print(f\"SORL Config: {sorl_config}\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Create Dummy Data ---\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "# Create a batch of random token sequences\n",
    "dummy_data = torch.randint(0, base_vocab_size, (batch_size, seq_len), device=sorl_model.model.device)\n",
    "print(f\"Created dummy data with shape: {dummy_data.shape}\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Test the `sorl_search` Function ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing `sorl_search` ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# This is the core of the SORL algorithm. It takes the original data and\n",
    "# finds a better representation by inserting abstract tokens.\n",
    "with torch.no_grad():\n",
    "    best_sequence, switch_ratio = sorl_search(dummy_data, sorl_model, sorl_config)\n",
    "\n",
    "print(f\"Original sequence length: {dummy_data.shape[1]}\")\n",
    "print(f\"Sequence length after search (with abstractions): {best_sequence.shape[1]}\")\n",
    "print(f\"Abstraction switch ratio: {switch_ratio:.2f}\")\n",
    "# The switch ratio indicates how often the algorithm preferred a sampled abstraction over the greedy one.\n",
    "assert best_sequence.shape[0] == batch_size\n",
    "assert best_sequence.shape[1] > seq_len # Should be longer due to added placeholders\n",
    "print(\"✅ `sorl_search` test passed.\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Test Loss Computation ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing Loss Computation ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute the loss on the improved sequence found by the search.\n",
    "# This is what would be used for the backward pass during training.\n",
    "ppt = compute_per_token_loss(sorl_model, best_sequence)\n",
    "ssl_loss, abs_loss = compute_loss(best_sequence, sorl_model, ppt)\n",
    "total_loss = ssl_loss + abs_loss\n",
    "\n",
    "print(f\"Per-token loss shape: {ppt.shape}\")\n",
    "print(f\"Trajectory Loss (ssl_loss): {ssl_loss.item():.4f}\")\n",
    "print(f\"Abstraction Loss (abs_loss): {abs_loss.item():.4f}\")\n",
    "print(f\"Total Loss: {total_loss.item():.4f}\")\n",
    "assert ssl_loss.ndim == 0 and abs_loss.ndim == 0\n",
    "print(\"✅ Loss computation test passed.\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Test the `evaluate` Function ---\n",
    "print(\"=\"*80)\n",
    "print(\"--- Testing `evaluate` ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# This function simulates a validation step, comparing a greedy search\n",
    "# against a random search to measure the potential for improvement.\n",
    "with torch.no_grad():\n",
    "    greedy_ppl, improve_ppl_percent, _, _ = evaluate(dummy_data, sorl_model, n=4, config=sorl_config)\n",
    "\n",
    "print(f\"Greedy trajectory perplexity: {greedy_ppl.item():.4f}\")\n",
    "print(f\"Search improvement over greedy: {improve_ppl_percent.item():.2f}%\")\n",
    "print(\"✅ `evaluate` test passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2214dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load language modeling data inside & train on them ~ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ec688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
