{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f514e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication Dataset (cot included/not, reverse/not)\n",
    "# - (no cot, no reverse) 11 x 12 = <answer> 132 \n",
    "# - (cot, no reverse) 11 x 12 = 12 + 120 = <answer> 132\n",
    "# - (no cot, reverse) 11 x 21 = <answer> 231 \n",
    "# - (cot, reverse) 11 x 21 = 21 + 021 = <answer> 231 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7693f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n",
      "Model initialized on cpu with 28.34M parameters.\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from dataset.base import MemLoader\n",
    "from src.sorl import SORLConfig\n",
    "import os \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration (Mimicking command-line args)\n",
    "# ==============================================================================\n",
    "args = SimpleNamespace(\n",
    "    # --- Paths ---\n",
    "    train_data_path=\"dataset/multiply/multiply_2x2_train.bin\",\n",
    "    val_data_path=\"dataset/multiply/multiply_2x2_val.bin\",\n",
    "    \n",
    "    # --- Model Config ---\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=2,\n",
    "    abstract_vocab_sizes=\"8\",\n",
    "    \n",
    "    # --- Training Config ---\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-4,\n",
    "    epoch=3,\n",
    "    \n",
    "    # --- SORL Config ---\n",
    "    n_rollout=5,\n",
    "    temperature=1.0,\n",
    "    K=4,\n",
    "    denoise_steps=1,\n",
    "    max_t_search=0,\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False,\n",
    "    use_special_placeholders=False,\n",
    "    special_token_id=31,\n",
    "    abstract_budget=5,\n",
    "    temperature_flip=False,\n",
    "    \n",
    "    # --- Curriculum and Memory ---\n",
    "    curriculum_ratio=0.6, # looks redundant as of now, it (vaguely) violates the \"compositionality\" principle\n",
    "    use_fade_memory=False,\n",
    "    use_compression_mask=False, # <-- Set to True to test your new mask\n",
    "    compression_curriculum_ratio=0.25,\n",
    "    memory_span=128,\n",
    "    \n",
    "    # --- GAPT ---\n",
    "    default_phase=None, # Set to 1 or 2 to override, None to enable GAPT\n",
    "    delta=0.01,\n",
    "    tau=0.1,\n",
    "    p_m=10,\n",
    "    p_c=10\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "val_loader = MemLoader(args.val_data_path, device=args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_loader.tokenizer_path) # data is tokenized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = len(tokenizer)\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "# 2 layer 4 head\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=args.memory_span,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model initialized on {args.device} with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")\n",
    "\n",
    "# --- SORL Config and Schedulers ---\n",
    "\n",
    "sorl_config = SORLConfig(\n",
    "    n=args.n_rollout, \n",
    "    temperature=args.temperature, \n",
    "    K=args.K,\n",
    "    l=1, \n",
    "    steps=args.denoise_steps, \n",
    "    max_t_search=args.max_t_search,\n",
    "    use_rhythmic_placeholders=args.use_rhythmic_placeholders,\n",
    "    use_spike_placeholders=args.use_spike_placeholders,\n",
    "    use_special_placeholders=args.use_special_placeholders,\n",
    "    special_token_id=args.special_token_id,\n",
    "    abstract_budget=args.abstract_budget,\n",
    "    temperature_flip=args.temperature_flip,\n",
    "    curriculum_ratio=args.curriculum_ratio,\n",
    "    use_fade_memory=args.use_fade_memory,\n",
    "    use_compression_mask=args.use_compression_mask,\n",
    "    min_keep=args.memory_span, \n",
    "    max_seq_len=train_loader.max_length,\n",
    "    train_iterations=int(args.epoch * len(train_loader) / args.batch_size), \n",
    "    train_batch_size=args.batch_size,\n",
    "    val_batch_size=args.batch_size,\n",
    "    max_length=train_loader.max_length,\n",
    "    default_phase=args.default_phase, \n",
    "    delta=args.delta, tau=args.tau,\n",
    "    p_m=args.p_m, p_c=args.p_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dce59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- No abstraction allowed\n",
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 3.46 (SSL: 3.461, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.8% | Abs-Free-Loss: 1.351 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 02 | Loss: 2.66 (SSL: 2.664, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 1.307 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 03 | Loss: 2.56 (SSL: 2.557, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 1.212 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 04 | Loss: 2.34 (SSL: 2.340, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 1.102 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 05 | Loss: 2.13 (SSL: 2.131, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 1.077 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 06 | Loss: 2.09 (SSL: 2.093, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 1.007 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 07 | Loss: 1.91 (SSL: 1.908, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.968 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 08 | Loss: 1.87 (SSL: 1.871, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.963 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 09 | Loss: 1.79 (SSL: 1.793, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -85.6% | Abs-Free-Loss: 0.953 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 10 | Loss: 1.77 (SSL: 1.768, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.903 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 11 | Loss: 1.78 (SSL: 1.782, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.898 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 12 | Loss: 1.68 (SSL: 1.679, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.877 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 13 | Loss: 1.62 (SSL: 1.618, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.847 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 14 | Loss: 1.67 (SSL: 1.667, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.837 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 15 | Loss: 1.64 (SSL: 1.640, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.839 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 16 | Loss: 1.60 (SSL: 1.601, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.843 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 17 | Loss: 1.59 (SSL: 1.589, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.803 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 18 | Loss: 1.56 (SSL: 1.556, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.816 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 19 | Loss: 1.53 (SSL: 1.531, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.792 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 20 | Loss: 1.54 (SSL: 1.539, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.796 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 21 | Loss: 1.50 (SSL: 1.501, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.787 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 22 | Loss: 1.51 (SSL: 1.513, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.798 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 23 | Loss: 1.53 (SSL: 1.533, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.794 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 24 | Loss: 1.53 (SSL: 1.526, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.773 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 25 | Loss: 1.48 (SSL: 1.478, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.786 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 26 | Loss: 1.51 (SSL: 1.505, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.760 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 27 | Loss: 1.45 (SSL: 1.451, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.772 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 28 | Loss: 1.48 (SSL: 1.481, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.782 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 29 | Loss: 1.46 (SSL: 1.459, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.757 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 30 | Loss: 1.46 (SSL: 1.455, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.3% | Abs-Free-Loss: 0.784 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 31 | Loss: 1.42 (SSL: 1.417, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.733 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 32 | Loss: 1.40 (SSL: 1.398, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.728 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 33 | Loss: 1.43 (SSL: 1.431, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.747 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 34 | Loss: 1.42 (SSL: 1.421, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.740 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 35 | Loss: 1.42 (SSL: 1.415, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.746 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 36 | Loss: 1.41 (SSL: 1.410, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.739 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 37 | Loss: 1.44 (SSL: 1.435, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.745 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 38 | Loss: 1.39 (SSL: 1.390, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.742 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 39 | Loss: 1.44 (SSL: 1.438, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.736 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 40 | Loss: 1.40 (SSL: 1.404, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.733 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 41 | Loss: 1.38 (SSL: 1.381, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.730 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 42 | Loss: 1.42 (SSL: 1.421, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.736 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 43 | Loss: 1.39 (SSL: 1.389, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.731 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 44 | Loss: 1.38 (SSL: 1.377, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.717 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 45 | Loss: 1.38 (SSL: 1.382, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.712 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 46 | Loss: 1.37 (SSL: 1.372, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.714 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 47 | Loss: 1.41 (SSL: 1.414, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.736 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 48 | Loss: 1.37 (SSL: 1.374, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.721 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 49 | Loss: 1.39 (SSL: 1.386, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.723 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 50 | Loss: 1.38 (SSL: 1.377, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.727 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 51 | Loss: 1.36 (SSL: 1.364, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.715 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 52 | Loss: 1.42 (SSL: 1.416, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.738 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 53 | Loss: 1.38 (SSL: 1.381, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.725 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 54 | Loss: 1.35 (SSL: 1.350, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.717 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 55 | Loss: 1.32 (SSL: 1.318, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.689 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 56 | Loss: 1.33 (SSL: 1.330, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.696 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 57 | Loss: 1.36 (SSL: 1.364, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.708 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 58 | Loss: 1.35 (SSL: 1.347, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.716 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 59 | Loss: 1.37 (SSL: 1.365, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.707 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 60 | Loss: 1.36 (SSL: 1.365, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.710 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 61 | Loss: 1.32 (SSL: 1.316, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.695 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 62 | Loss: 1.37 (SSL: 1.366, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.714 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 63 | Loss: 1.33 (SSL: 1.330, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.693 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 64 | Loss: 1.32 (SSL: 1.322, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.687 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 65 | Loss: 1.33 (SSL: 1.326, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.687 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 66 | Loss: 1.34 (SSL: 1.340, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.697 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 67 | Loss: 1.37 (SSL: 1.366, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.710 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 68 | Loss: 1.32 (SSL: 1.319, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.683 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 69 | Loss: 1.32 (SSL: 1.318, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.681 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 70 | Loss: 1.34 (SSL: 1.345, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.696 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 71 | Loss: 1.32 (SSL: 1.317, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.680 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 72 | Loss: 1.34 (SSL: 1.339, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.695 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 73 | Loss: 1.32 (SSL: 1.324, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.699 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 74 | Loss: 1.28 (SSL: 1.277, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.669 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 75 | Loss: 1.32 (SSL: 1.316, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.686 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 76 | Loss: 1.26 (SSL: 1.257, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.654 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 77 | Loss: 1.27 (SSL: 1.273, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.657 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 78 | Loss: 1.29 (SSL: 1.289, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.674 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 79 | Loss: 1.29 (SSL: 1.292, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.672 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 80 | Loss: 1.29 (SSL: 1.286, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.659 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 81 | Loss: 1.27 (SSL: 1.273, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.668 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 82 | Loss: 1.27 (SSL: 1.270, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -94.2% | Abs-Free-Loss: 0.650 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 83 | Loss: 1.27 (SSL: 1.273, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.655 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 84 | Loss: 1.29 (SSL: 1.290, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.667 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 85 | Loss: 1.26 (SSL: 1.255, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.653 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 86 | Loss: 1.27 (SSL: 1.274, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.667 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 87 | Loss: 1.26 (SSL: 1.255, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.650 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 88 | Loss: 1.25 (SSL: 1.248, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.652 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 89 | Loss: 1.24 (SSL: 1.245, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.650 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 90 | Loss: 1.24 (SSL: 1.244, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.648 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 91 | Loss: 1.30 (SSL: 1.296, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.677 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 92 | Loss: 1.23 (SSL: 1.228, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.635 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 93 | Loss: 1.25 (SSL: 1.254, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.655 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 94 | Loss: 1.25 (SSL: 1.255, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.648 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 95 | Loss: 1.23 (SSL: 1.233, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.634 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 96 | Loss: 1.23 (SSL: 1.230, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.631 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 97 | Loss: 1.25 (SSL: 1.246, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.649 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 98 | Loss: 1.23 (SSL: 1.230, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.640 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 99 | Loss: 1.20 (SSL: 1.202, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.623 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 100 | Loss: 1.22 (SSL: 1.219, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.623 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 101 | Loss: 1.21 (SSL: 1.215, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.631 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 102 | Loss: 1.22 (SSL: 1.224, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.628 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 103 | Loss: 1.18 (SSL: 1.178, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.613 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 104 | Loss: 1.23 (SSL: 1.228, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.633 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 105 | Loss: 1.19 (SSL: 1.187, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.610 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 106 | Loss: 1.19 (SSL: 1.187, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.622 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 107 | Loss: 1.21 (SSL: 1.209, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.617 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 108 | Loss: 1.19 (SSL: 1.189, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.618 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 109 | Loss: 1.21 (SSL: 1.206, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.618 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 110 | Loss: 1.21 (SSL: 1.206, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.609 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 111 | Loss: 1.14 (SSL: 1.141, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.600 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 112 | Loss: 1.15 (SSL: 1.155, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.595 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 113 | Loss: 1.20 (SSL: 1.199, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.612 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 114 | Loss: 1.17 (SSL: 1.170, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.606 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 115 | Loss: 1.15 (SSL: 1.149, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.589 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 116 | Loss: 1.13 (SSL: 1.131, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.580 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 117 | Loss: 1.15 (SSL: 1.151, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.601 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 118 | Loss: 1.14 (SSL: 1.143, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.578 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 119 | Loss: 1.15 (SSL: 1.145, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.587 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 120 | Loss: 1.11 (SSL: 1.113, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.580 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 121 | Loss: 1.13 (SSL: 1.128, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.578 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 122 | Loss: 1.13 (SSL: 1.129, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.582 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 123 | Loss: 1.10 (SSL: 1.101, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.564 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 124 | Loss: 1.14 (SSL: 1.139, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.587 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 125 | Loss: 1.12 (SSL: 1.123, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.565 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 126 | Loss: 1.06 (SSL: 1.064, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.551 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 127 | Loss: 1.03 (SSL: 1.030, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.536 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 128 | Loss: 1.09 (SSL: 1.092, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.553 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 129 | Loss: 1.11 (SSL: 1.109, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.557 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 130 | Loss: 1.08 (SSL: 1.076, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.557 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 131 | Loss: 1.09 (SSL: 1.087, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.558 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 132 | Loss: 1.12 (SSL: 1.124, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.1% | Abs-Free-Loss: 0.565 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 133 | Loss: 1.02 (SSL: 1.021, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.529 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 134 | Loss: 1.05 (SSL: 1.055, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.544 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 135 | Loss: 1.07 (SSL: 1.074, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.9% | Abs-Free-Loss: 0.537 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 136 | Loss: 1.05 (SSL: 1.046, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.542 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 137 | Loss: 0.99 (SSL: 0.992, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.498 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 138 | Loss: 1.05 (SSL: 1.051, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.544 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 139 | Loss: 1.08 (SSL: 1.079, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.551 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 140 | Loss: 1.07 (SSL: 1.074, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.535 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 141 | Loss: 1.06 (SSL: 1.062, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.549 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 142 | Loss: 1.03 (SSL: 1.033, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.534 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 143 | Loss: 1.06 (SSL: 1.056, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.520 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 144 | Loss: 1.05 (SSL: 1.048, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.534 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 145 | Loss: 0.96 (SSL: 0.964, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.496 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 146 | Loss: 1.04 (SSL: 1.038, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.533 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 147 | Loss: 1.03 (SSL: 1.030, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.8% | Abs-Free-Loss: 0.517 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 148 | Loss: 1.03 (SSL: 1.034, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.524 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 149 | Loss: 1.04 (SSL: 1.036, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.529 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 150 | Loss: 0.99 (SSL: 0.991, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.500 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 151 | Loss: 1.02 (SSL: 1.020, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.524 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 152 | Loss: 1.01 (SSL: 1.007, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.513 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 153 | Loss: 0.99 (SSL: 0.986, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.507 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 154 | Loss: 0.97 (SSL: 0.973, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.501 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 155 | Loss: 1.01 (SSL: 1.012, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.518 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 156 | Loss: 0.98 (SSL: 0.981, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.500 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 157 | Loss: 0.97 (SSL: 0.970, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.498 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 158 | Loss: 0.99 (SSL: 0.986, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.504 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 159 | Loss: 0.99 (SSL: 0.993, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.8% | Abs-Free-Loss: 0.498 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 160 | Loss: 0.99 (SSL: 0.991, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.507 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 161 | Loss: 0.98 (SSL: 0.976, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.494 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 162 | Loss: 0.96 (SSL: 0.961, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.495 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 163 | Loss: 0.92 (SSL: 0.919, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.469 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 164 | Loss: 0.97 (SSL: 0.973, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.503 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 165 | Loss: 0.99 (SSL: 0.992, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.508 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 166 | Loss: 0.96 (SSL: 0.955, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.482 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 167 | Loss: 0.96 (SSL: 0.961, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.497 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 168 | Loss: 0.97 (SSL: 0.966, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.486 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 169 | Loss: 0.92 (SSL: 0.919, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.472 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 170 | Loss: 0.94 (SSL: 0.936, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.474 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 171 | Loss: 0.98 (SSL: 0.983, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.504 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 172 | Loss: 0.94 (SSL: 0.937, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.473 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 173 | Loss: 0.91 (SSL: 0.911, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.463 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 174 | Loss: 0.93 (SSL: 0.925, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.459 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 175 | Loss: 0.94 (SSL: 0.937, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.477 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 176 | Loss: 0.92 (SSL: 0.924, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.467 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 177 | Loss: 0.94 (SSL: 0.943, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.479 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 178 | Loss: 0.93 (SSL: 0.932, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.467 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 179 | Loss: 0.90 (SSL: 0.896, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.457 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 180 | Loss: 0.88 (SSL: 0.882, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.456 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 181 | Loss: 0.95 (SSL: 0.947, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.483 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 182 | Loss: 0.93 (SSL: 0.932, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.470 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 183 | Loss: 0.93 (SSL: 0.926, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.478 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 184 | Loss: 0.86 (SSL: 0.855, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.438 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 185 | Loss: 0.92 (SSL: 0.918, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.468 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 186 | Loss: 0.91 (SSL: 0.915, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.469 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 187 | Loss: 0.86 (SSL: 0.860, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.436 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 188 | Loss: 0.95 (SSL: 0.947, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.474 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 189 | Loss: 0.86 (SSL: 0.864, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.443 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 190 | Loss: 0.90 (SSL: 0.902, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.454 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 191 | Loss: 0.91 (SSL: 0.906, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.457 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 192 | Loss: 0.87 (SSL: 0.870, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.446 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 193 | Loss: 0.86 (SSL: 0.860, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.444 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 194 | Loss: 0.90 (SSL: 0.904, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.455 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 195 | Loss: 0.86 (SSL: 0.859, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.442 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 196 | Loss: 0.85 (SSL: 0.846, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.427 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 197 | Loss: 0.86 (SSL: 0.860, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.433 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 198 | Loss: 0.88 (SSL: 0.884, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.453 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 199 | Loss: 0.87 (SSL: 0.867, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.436 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 200 | Loss: 0.88 (SSL: 0.878, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.450 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 201 | Loss: 0.87 (SSL: 0.869, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.441 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 202 | Loss: 0.88 (SSL: 0.883, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.441 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 203 | Loss: 0.90 (SSL: 0.896, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.453 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 204 | Loss: 0.86 (SSL: 0.861, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.6% | Abs-Free-Loss: 0.448 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 205 | Loss: 0.84 (SSL: 0.842, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.417 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 206 | Loss: 0.88 (SSL: 0.881, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.442 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 207 | Loss: 0.86 (SSL: 0.862, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.430 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 208 | Loss: 0.85 (SSL: 0.853, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.444 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 209 | Loss: 0.84 (SSL: 0.841, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.433 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 210 | Loss: 0.87 (SSL: 0.866, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.430 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 211 | Loss: 0.87 (SSL: 0.870, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.437 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 212 | Loss: 0.84 (SSL: 0.843, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.430 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 213 | Loss: 0.87 (SSL: 0.873, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.445 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 214 | Loss: 0.85 (SSL: 0.845, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.430 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 215 | Loss: 0.83 (SSL: 0.826, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.414 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 216 | Loss: 0.83 (SSL: 0.827, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.425 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 217 | Loss: 0.87 (SSL: 0.870, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.439 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 218 | Loss: 0.86 (SSL: 0.859, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.6% | Abs-Free-Loss: 0.443 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 219 | Loss: 0.87 (SSL: 0.866, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.441 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 220 | Loss: 0.83 (SSL: 0.828, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.4% | Abs-Free-Loss: 0.417 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 221 | Loss: 0.85 (SSL: 0.854, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.433 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 222 | Loss: 0.80 (SSL: 0.799, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.409 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 223 | Loss: 0.80 (SSL: 0.797, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.403 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 224 | Loss: 0.81 (SSL: 0.808, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.412 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 225 | Loss: 0.78 (SSL: 0.780, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.410 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 226 | Loss: 0.83 (SSL: 0.831, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.424 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 227 | Loss: 0.85 (SSL: 0.846, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.430 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 228 | Loss: 0.81 (SSL: 0.815, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.420 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 229 | Loss: 0.78 (SSL: 0.781, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.393 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 230 | Loss: 0.81 (SSL: 0.813, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.411 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 231 | Loss: 0.81 (SSL: 0.808, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.410 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 232 | Loss: 0.79 (SSL: 0.786, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.396 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 233 | Loss: 0.77 (SSL: 0.769, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.396 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 234 | Loss: 0.81 (SSL: 0.811, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.413 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 235 | Loss: 0.81 (SSL: 0.810, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.406 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 236 | Loss: 0.79 (SSL: 0.795, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.406 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 237 | Loss: 0.79 (SSL: 0.789, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.8% | Abs-Free-Loss: 0.396 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 238 | Loss: 0.82 (SSL: 0.822, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.422 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 239 | Loss: 0.85 (SSL: 0.854, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.424 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 240 | Loss: 0.83 (SSL: 0.825, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.409 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 241 | Loss: 0.78 (SSL: 0.782, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.403 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 242 | Loss: 0.78 (SSL: 0.778, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.399 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 243 | Loss: 0.80 (SSL: 0.797, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.410 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 244 | Loss: 0.85 (SSL: 0.848, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.429 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 245 | Loss: 0.82 (SSL: 0.823, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.420 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 246 | Loss: 0.78 (SSL: 0.782, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.401 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 247 | Loss: 0.76 (SSL: 0.762, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.1% | Abs-Free-Loss: 0.398 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 248 | Loss: 0.79 (SSL: 0.795, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.402 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 249 | Loss: 0.85 (SSL: 0.846, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.6% | Abs-Free-Loss: 0.431 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 250 | Loss: 0.75 (SSL: 0.747, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.382 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 251 | Loss: 0.75 (SSL: 0.752, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.386 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 252 | Loss: 0.75 (SSL: 0.755, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.387 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 253 | Loss: 0.76 (SSL: 0.765, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.396 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 254 | Loss: 0.77 (SSL: 0.770, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.394 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 255 | Loss: 0.78 (SSL: 0.780, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.399 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 256 | Loss: 0.78 (SSL: 0.777, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.400 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 257 | Loss: 0.77 (SSL: 0.772, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.382 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 258 | Loss: 0.76 (SSL: 0.764, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.390 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 259 | Loss: 0.80 (SSL: 0.799, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.403 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 260 | Loss: 0.77 (SSL: 0.771, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.396 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 261 | Loss: 0.78 (SSL: 0.784, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.402 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 262 | Loss: 0.74 (SSL: 0.745, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.378 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 263 | Loss: 0.70 (SSL: 0.703, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.360 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 264 | Loss: 0.77 (SSL: 0.770, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.1% | Abs-Free-Loss: 0.390 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 265 | Loss: 0.80 (SSL: 0.796, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.400 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 266 | Loss: 0.75 (SSL: 0.750, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.381 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 267 | Loss: 0.76 (SSL: 0.759, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.394 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 268 | Loss: 0.75 (SSL: 0.752, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.389 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 269 | Loss: 0.74 (SSL: 0.744, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.376 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 270 | Loss: 0.75 (SSL: 0.746, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.376 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 271 | Loss: 0.77 (SSL: 0.774, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.395 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 272 | Loss: 0.79 (SSL: 0.793, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.3% | Abs-Free-Loss: 0.409 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 273 | Loss: 0.81 (SSL: 0.806, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.414 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 274 | Loss: 0.77 (SSL: 0.771, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.396 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 275 | Loss: 0.77 (SSL: 0.772, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.382 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 276 | Loss: 0.77 (SSL: 0.765, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.401 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 277 | Loss: 0.77 (SSL: 0.766, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.393 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 278 | Loss: 0.81 (SSL: 0.812, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.406 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 279 | Loss: 0.80 (SSL: 0.799, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.403 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 280 | Loss: 0.73 (SSL: 0.732, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.376 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 281 | Loss: 0.75 (SSL: 0.755, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.385 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 282 | Loss: 0.80 (SSL: 0.795, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.398 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 283 | Loss: 0.76 (SSL: 0.755, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.386 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 284 | Loss: 0.75 (SSL: 0.749, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.385 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 285 | Loss: 0.73 (SSL: 0.729, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.374 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 286 | Loss: 0.74 (SSL: 0.743, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.383 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 287 | Loss: 0.75 (SSL: 0.749, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.381 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 288 | Loss: 0.77 (SSL: 0.767, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.391 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 289 | Loss: 0.74 (SSL: 0.744, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.376 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 290 | Loss: 0.78 (SSL: 0.781, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.389 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 291 | Loss: 0.67 (SSL: 0.670, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.348 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 292 | Loss: 0.75 (SSL: 0.747, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.382 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 293 | Loss: 0.76 (SSL: 0.757, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.388 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 294 | Loss: 0.75 (SSL: 0.753, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.386 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 295 | Loss: 0.73 (SSL: 0.730, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.371 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 296 | Loss: 0.71 (SSL: 0.707, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.357 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 297 | Loss: 0.73 (SSL: 0.730, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.378 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 298 | Loss: 0.77 (SSL: 0.769, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.383 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 299 | Loss: 0.76 (SSL: 0.755, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.378 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 300 | Loss: 0.72 (SSL: 0.723, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.364 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 301 | Loss: 0.71 (SSL: 0.706, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.367 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 302 | Loss: 0.75 (SSL: 0.755, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.393 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 303 | Loss: 0.76 (SSL: 0.757, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.390 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 304 | Loss: 0.78 (SSL: 0.777, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.389 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 305 | Loss: 0.74 (SSL: 0.742, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.378 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 306 | Loss: 0.72 (SSL: 0.718, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.373 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 307 | Loss: 0.72 (SSL: 0.719, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.366 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 308 | Loss: 0.72 (SSL: 0.722, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.374 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 309 | Loss: 0.73 (SSL: 0.725, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.368 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 310 | Loss: 0.70 (SSL: 0.697, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.1% | Abs-Free-Loss: 0.350 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 311 | Loss: 0.72 (SSL: 0.724, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.380 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 312 | Loss: 0.72 (SSL: 0.716, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.366 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 313 | Loss: 0.70 (SSL: 0.696, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.353 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 314 | Loss: 0.67 (SSL: 0.669, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.7% | Abs-Free-Loss: 0.337 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 315 | Loss: 0.75 (SSL: 0.745, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.383 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 316 | Loss: 0.72 (SSL: 0.718, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.374 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 317 | Loss: 0.70 (SSL: 0.699, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.364 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 318 | Loss: 0.69 (SSL: 0.686, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.354 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 319 | Loss: 0.72 (SSL: 0.718, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.368 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 320 | Loss: 0.70 (SSL: 0.703, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.362 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 321 | Loss: 0.71 (SSL: 0.707, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.365 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 322 | Loss: 0.70 (SSL: 0.696, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.360 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 323 | Loss: 0.70 (SSL: 0.698, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.359 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 324 | Loss: 0.75 (SSL: 0.753, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.386 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 325 | Loss: 0.72 (SSL: 0.716, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.367 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 326 | Loss: 0.64 (SSL: 0.642, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.325 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 327 | Loss: 0.73 (SSL: 0.726, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.375 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 328 | Loss: 0.68 (SSL: 0.684, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.349 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 329 | Loss: 0.72 (SSL: 0.716, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.3% | Abs-Free-Loss: 0.375 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 330 | Loss: 0.70 (SSL: 0.700, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.361 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 331 | Loss: 0.68 (SSL: 0.684, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.353 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 332 | Loss: 0.70 (SSL: 0.699, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.365 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 333 | Loss: 0.70 (SSL: 0.699, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.364 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 334 | Loss: 0.73 (SSL: 0.734, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.6% | Abs-Free-Loss: 0.375 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 335 | Loss: 0.68 (SSL: 0.679, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.347 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 336 | Loss: 0.72 (SSL: 0.724, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.375 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 337 | Loss: 0.67 (SSL: 0.667, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.345 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 338 | Loss: 0.71 (SSL: 0.707, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.365 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 339 | Loss: 0.69 (SSL: 0.687, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.355 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 340 | Loss: 0.72 (SSL: 0.717, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.361 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 341 | Loss: 0.68 (SSL: 0.683, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.351 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 342 | Loss: 0.68 (SSL: 0.677, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.348 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 343 | Loss: 0.66 (SSL: 0.655, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.338 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 344 | Loss: 0.70 (SSL: 0.700, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.358 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 345 | Loss: 0.69 (SSL: 0.690, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.346 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 346 | Loss: 0.69 (SSL: 0.694, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.364 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 347 | Loss: 0.71 (SSL: 0.711, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.371 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 348 | Loss: 0.71 (SSL: 0.710, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.359 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 349 | Loss: 0.70 (SSL: 0.696, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.355 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 350 | Loss: 0.72 (SSL: 0.720, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.369 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 351 | Loss: 0.68 (SSL: 0.681, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.353 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 352 | Loss: 0.71 (SSL: 0.708, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.357 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 353 | Loss: 0.72 (SSL: 0.718, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.362 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 354 | Loss: 0.68 (SSL: 0.684, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.355 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 355 | Loss: 0.67 (SSL: 0.666, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.344 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 356 | Loss: 0.69 (SSL: 0.691, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.359 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 357 | Loss: 0.72 (SSL: 0.719, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.371 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 358 | Loss: 0.68 (SSL: 0.676, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.354 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 359 | Loss: 0.68 (SSL: 0.684, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.346 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 360 | Loss: 0.68 (SSL: 0.679, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.350 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 361 | Loss: 0.70 (SSL: 0.695, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.354 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 362 | Loss: 0.66 (SSL: 0.664, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.343 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 363 | Loss: 0.64 (SSL: 0.639, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.328 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 364 | Loss: 0.68 (SSL: 0.676, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.347 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 365 | Loss: 0.65 (SSL: 0.651, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.337 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 366 | Loss: 0.69 (SSL: 0.686, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.357 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 367 | Loss: 0.69 (SSL: 0.693, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.357 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 368 | Loss: 0.69 (SSL: 0.689, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.353 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 369 | Loss: 0.72 (SSL: 0.720, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.373 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 370 | Loss: 0.68 (SSL: 0.680, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.353 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 371 | Loss: 0.66 (SSL: 0.659, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 372 | Loss: 0.71 (SSL: 0.706, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.356 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 373 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.337 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 374 | Loss: 0.65 (SSL: 0.649, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.333 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 375 | Loss: 0.64 (SSL: 0.640, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.333 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 376 | Loss: 0.69 (SSL: 0.691, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.357 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 377 | Loss: 0.69 (SSL: 0.694, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.355 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 378 | Loss: 0.67 (SSL: 0.675, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.350 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 379 | Loss: 0.69 (SSL: 0.691, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.354 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 380 | Loss: 0.70 (SSL: 0.702, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.361 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 381 | Loss: 0.71 (SSL: 0.713, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.369 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 382 | Loss: 0.66 (SSL: 0.659, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.345 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 383 | Loss: 0.66 (SSL: 0.659, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.343 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 384 | Loss: 0.69 (SSL: 0.689, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.349 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 385 | Loss: 0.64 (SSL: 0.641, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 386 | Loss: 0.67 (SSL: 0.665, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.346 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 387 | Loss: 0.66 (SSL: 0.656, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.335 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 388 | Loss: 0.66 (SSL: 0.661, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.344 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 389 | Loss: 0.70 (SSL: 0.699, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.365 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 390 | Loss: 0.63 (SSL: 0.630, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.328 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 391 | Loss: 0.71 (SSL: 0.708, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.368 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 392 | Loss: 0.71 (SSL: 0.711, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.363 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 393 | Loss: 0.62 (SSL: 0.623, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.320 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 394 | Loss: 0.64 (SSL: 0.640, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.329 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 395 | Loss: 0.63 (SSL: 0.625, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.327 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 396 | Loss: 0.67 (SSL: 0.668, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.351 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 397 | Loss: 0.71 (SSL: 0.708, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.370 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 398 | Loss: 0.62 (SSL: 0.623, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.7% | Abs-Free-Loss: 0.309 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 399 | Loss: 0.65 (SSL: 0.655, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.341 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 400 | Loss: 0.66 (SSL: 0.661, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.337 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 401 | Loss: 0.68 (SSL: 0.685, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.349 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 402 | Loss: 0.66 (SSL: 0.656, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 403 | Loss: 0.65 (SSL: 0.654, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 404 | Loss: 0.64 (SSL: 0.638, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.334 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 405 | Loss: 0.66 (SSL: 0.659, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.345 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 406 | Loss: 0.67 (SSL: 0.671, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.340 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 407 | Loss: 0.66 (SSL: 0.655, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.335 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 408 | Loss: 0.64 (SSL: 0.641, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.330 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 409 | Loss: 0.67 (SSL: 0.665, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.350 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 410 | Loss: 0.63 (SSL: 0.627, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.320 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 411 | Loss: 0.67 (SSL: 0.669, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.344 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 412 | Loss: 0.67 (SSL: 0.665, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.348 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 413 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 414 | Loss: 0.68 (SSL: 0.676, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.351 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 415 | Loss: 0.67 (SSL: 0.672, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.352 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 416 | Loss: 0.71 (SSL: 0.708, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.361 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 417 | Loss: 0.68 (SSL: 0.676, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.347 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 418 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.329 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 419 | Loss: 0.69 (SSL: 0.689, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.358 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 420 | Loss: 0.67 (SSL: 0.668, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.348 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 421 | Loss: 0.66 (SSL: 0.662, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.342 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 422 | Loss: 0.64 (SSL: 0.639, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.326 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 423 | Loss: 0.65 (SSL: 0.651, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 424 | Loss: 0.66 (SSL: 0.662, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.342 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 425 | Loss: 0.67 (SSL: 0.672, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.347 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 426 | Loss: 0.66 (SSL: 0.662, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.1% | Abs-Free-Loss: 0.341 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 427 | Loss: 0.67 (SSL: 0.672, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.347 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 428 | Loss: 0.65 (SSL: 0.646, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.336 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 429 | Loss: 0.65 (SSL: 0.647, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.331 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 430 | Loss: 0.62 (SSL: 0.621, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.324 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 431 | Loss: 0.63 (SSL: 0.632, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.1% | Abs-Free-Loss: 0.319 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 432 | Loss: 0.64 (SSL: 0.638, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.333 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 433 | Loss: 0.67 (SSL: 0.667, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.342 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 434 | Loss: 0.68 (SSL: 0.680, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.349 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 435 | Loss: 0.69 (SSL: 0.690, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.355 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 436 | Loss: 0.68 (SSL: 0.679, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.362 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 437 | Loss: 0.69 (SSL: 0.690, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.361 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 438 | Loss: 0.68 (SSL: 0.683, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.342 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 439 | Loss: 0.68 (SSL: 0.685, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.359 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 440 | Loss: 0.66 (SSL: 0.664, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.344 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 441 | Loss: 0.69 (SSL: 0.695, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.363 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 442 | Loss: 0.66 (SSL: 0.656, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.338 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 443 | Loss: 0.63 (SSL: 0.635, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.324 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 444 | Loss: 0.65 (SSL: 0.651, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 445 | Loss: 0.63 (SSL: 0.633, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.323 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 446 | Loss: 0.64 (SSL: 0.636, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.330 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 447 | Loss: 0.66 (SSL: 0.663, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.340 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 448 | Loss: 0.64 (SSL: 0.645, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.334 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 449 | Loss: 0.63 (SSL: 0.628, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.326 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 450 | Loss: 0.68 (SSL: 0.680, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.349 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 451 | Loss: 0.65 (SSL: 0.654, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.337 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 452 | Loss: 0.65 (SSL: 0.646, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.322 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 453 | Loss: 0.66 (SSL: 0.664, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.342 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 454 | Loss: 0.61 (SSL: 0.607, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.322 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 455 | Loss: 0.66 (SSL: 0.660, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.341 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 456 | Loss: 0.68 (SSL: 0.684, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.348 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 457 | Loss: 0.63 (SSL: 0.628, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.9% | Abs-Free-Loss: 0.328 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 458 | Loss: 0.59 (SSL: 0.587, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.300 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 459 | Loss: 0.61 (SSL: 0.612, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.316 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 460 | Loss: 0.65 (SSL: 0.651, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.329 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 461 | Loss: 0.64 (SSL: 0.643, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 462 | Loss: 0.63 (SSL: 0.632, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.331 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 463 | Loss: 0.69 (SSL: 0.690, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -86.2% | Abs-Free-Loss: 0.361 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 464 | Loss: 0.60 (SSL: 0.602, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.306 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 465 | Loss: 0.65 (SSL: 0.654, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.334 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 466 | Loss: 0.62 (SSL: 0.619, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.316 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 467 | Loss: 0.65 (SSL: 0.650, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.338 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 468 | Loss: 0.64 (SSL: 0.643, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.5% | Abs-Free-Loss: 0.323 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 469 | Loss: 0.62 (SSL: 0.620, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.308 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 470 | Loss: 0.64 (SSL: 0.642, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.327 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 471 | Loss: 0.65 (SSL: 0.647, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.334 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 472 | Loss: 0.62 (SSL: 0.616, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.315 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 473 | Loss: 0.67 (SSL: 0.675, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.338 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 474 | Loss: 0.65 (SSL: 0.648, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.331 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 475 | Loss: 0.68 (SSL: 0.676, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.345 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 476 | Loss: 0.63 (SSL: 0.628, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.320 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 477 | Loss: 0.67 (SSL: 0.669, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.347 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 478 | Loss: 0.66 (SSL: 0.661, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.333 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 479 | Loss: 0.64 (SSL: 0.642, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.331 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 480 | Loss: 0.61 (SSL: 0.606, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.311 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 481 | Loss: 0.63 (SSL: 0.631, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.320 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 482 | Loss: 0.65 (SSL: 0.651, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -87.6% | Abs-Free-Loss: 0.336 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 483 | Loss: 0.62 (SSL: 0.615, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.311 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 484 | Loss: 0.62 (SSL: 0.621, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.322 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 485 | Loss: 0.60 (SSL: 0.596, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.1% | Abs-Free-Loss: 0.308 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 486 | Loss: 0.65 (SSL: 0.647, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.331 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 487 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.327 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 488 | Loss: 0.64 (SSL: 0.644, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 489 | Loss: 0.62 (SSL: 0.625, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.3% | Abs-Free-Loss: 0.326 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 490 | Loss: 0.62 (SSL: 0.625, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.319 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 491 | Loss: 0.65 (SSL: 0.653, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.333 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 492 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.335 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 493 | Loss: 0.63 (SSL: 0.633, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.319 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 494 | Loss: 0.58 (SSL: 0.578, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.292 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 495 | Loss: 0.63 (SSL: 0.628, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.317 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 496 | Loss: 0.69 (SSL: 0.686, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.335 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 497 | Loss: 0.55 (SSL: 0.552, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.8% | Abs-Free-Loss: 0.276 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 498 | Loss: 0.66 (SSL: 0.659, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.327 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 499 | Loss: 0.65 (SSL: 0.647, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 500 | Loss: 0.65 (SSL: 0.646, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.326 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 501 | Loss: 0.61 (SSL: 0.608, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.306 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 502 | Loss: 0.66 (SSL: 0.662, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.340 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 503 | Loss: 0.61 (SSL: 0.612, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.310 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 504 | Loss: 0.66 (SSL: 0.660, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 505 | Loss: 0.62 (SSL: 0.621, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.308 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 506 | Loss: 0.62 (SSL: 0.621, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.310 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 507 | Loss: 0.51 (SSL: 0.510, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.262 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 508 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.8% | Abs-Free-Loss: 0.330 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 509 | Loss: 0.59 (SSL: 0.593, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.301 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 510 | Loss: 0.62 (SSL: 0.616, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.321 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 511 | Loss: 0.62 (SSL: 0.623, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.307 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 512 | Loss: 0.67 (SSL: 0.672, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 513 | Loss: 0.63 (SSL: 0.627, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.322 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 514 | Loss: 0.63 (SSL: 0.631, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.323 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 515 | Loss: 0.67 (SSL: 0.666, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.338 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 516 | Loss: 0.59 (SSL: 0.590, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.299 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 517 | Loss: 0.60 (SSL: 0.603, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.305 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 518 | Loss: 0.65 (SSL: 0.653, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.330 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 519 | Loss: 0.61 (SSL: 0.608, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.310 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 520 | Loss: 0.61 (SSL: 0.614, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.313 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 521 | Loss: 0.59 (SSL: 0.593, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.300 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 522 | Loss: 0.63 (SSL: 0.629, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.315 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 523 | Loss: 0.60 (SSL: 0.601, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.300 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 524 | Loss: 0.62 (SSL: 0.616, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.319 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 525 | Loss: 0.63 (SSL: 0.633, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.319 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 526 | Loss: 0.60 (SSL: 0.601, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.304 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 527 | Loss: 0.57 (SSL: 0.569, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.288 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 528 | Loss: 0.57 (SSL: 0.573, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.293 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 529 | Loss: 0.57 (SSL: 0.574, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.293 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 530 | Loss: 0.57 (SSL: 0.575, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.289 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 531 | Loss: 0.62 (SSL: 0.618, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.306 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 532 | Loss: 0.59 (SSL: 0.588, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.302 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 533 | Loss: 0.58 (SSL: 0.580, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.295 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 534 | Loss: 0.66 (SSL: 0.657, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.1% | Abs-Free-Loss: 0.339 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 535 | Loss: 0.62 (SSL: 0.617, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.308 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 536 | Loss: 0.62 (SSL: 0.619, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.316 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 537 | Loss: 0.61 (SSL: 0.612, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.310 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 538 | Loss: 0.55 (SSL: 0.547, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.276 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 539 | Loss: 0.56 (SSL: 0.562, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.287 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 540 | Loss: 0.62 (SSL: 0.618, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.295 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 541 | Loss: 0.60 (SSL: 0.598, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.1% | Abs-Free-Loss: 0.304 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 542 | Loss: 0.64 (SSL: 0.644, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -85.9% | Abs-Free-Loss: 0.332 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 543 | Loss: 0.51 (SSL: 0.510, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.262 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 544 | Loss: 0.60 (SSL: 0.604, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.9% | Abs-Free-Loss: 0.305 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 545 | Loss: 0.60 (SSL: 0.598, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.297 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 546 | Loss: 0.64 (SSL: 0.636, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.317 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 547 | Loss: 0.60 (SSL: 0.604, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.300 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 548 | Loss: 0.62 (SSL: 0.624, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.313 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 549 | Loss: 0.59 (SSL: 0.591, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.304 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 550 | Loss: 0.60 (SSL: 0.604, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.3% | Abs-Free-Loss: 0.308 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 551 | Loss: 0.58 (SSL: 0.580, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.294 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 552 | Loss: 0.55 (SSL: 0.554, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.277 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 553 | Loss: 0.58 (SSL: 0.582, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.293 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 554 | Loss: 0.58 (SSL: 0.583, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.290 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 555 | Loss: 0.59 (SSL: 0.591, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -86.8% | Abs-Free-Loss: 0.300 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 556 | Loss: 0.57 (SSL: 0.573, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.297 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 557 | Loss: 0.67 (SSL: 0.669, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.5% | Abs-Free-Loss: 0.331 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 558 | Loss: 0.63 (SSL: 0.634, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.309 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 559 | Loss: 0.60 (SSL: 0.601, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.2% | Abs-Free-Loss: 0.302 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 560 | Loss: 0.58 (SSL: 0.577, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.291 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 561 | Loss: 0.58 (SSL: 0.577, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.287 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 562 | Loss: 0.64 (SSL: 0.643, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.322 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 563 | Loss: 0.55 (SSL: 0.551, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.271 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 564 | Loss: 0.58 (SSL: 0.581, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.295 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 565 | Loss: 0.62 (SSL: 0.623, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.314 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 566 | Loss: 0.64 (SSL: 0.641, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.322 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 567 | Loss: 0.61 (SSL: 0.612, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.9% | Abs-Free-Loss: 0.300 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 568 | Loss: 0.58 (SSL: 0.584, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.1% | Abs-Free-Loss: 0.292 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 569 | Loss: 0.62 (SSL: 0.618, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.5% | Abs-Free-Loss: 0.318 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 570 | Loss: 0.58 (SSL: 0.583, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.297 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 571 | Loss: 0.59 (SSL: 0.590, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.3% | Abs-Free-Loss: 0.297 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 572 | Loss: 0.56 (SSL: 0.558, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.280 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 573 | Loss: 0.56 (SSL: 0.561, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.281 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 574 | Loss: 0.55 (SSL: 0.549, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.6% | Abs-Free-Loss: 0.275 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 575 | Loss: 0.58 (SSL: 0.576, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.292 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 576 | Loss: 0.59 (SSL: 0.590, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.292 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 577 | Loss: 0.59 (SSL: 0.595, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -88.8% | Abs-Free-Loss: 0.299 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 578 | Loss: 0.57 (SSL: 0.567, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.280 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 579 | Loss: 0.59 (SSL: 0.592, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -90.8% | Abs-Free-Loss: 0.299 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 580 | Loss: 0.56 (SSL: 0.560, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.286 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 581 | Loss: 0.58 (SSL: 0.577, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -90.2% | Abs-Free-Loss: 0.288 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 582 | Loss: 0.62 (SSL: 0.618, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.6% | Abs-Free-Loss: 0.312 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 583 | Loss: 0.57 (SSL: 0.567, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -92.2% | Abs-Free-Loss: 0.285 | t_search: 0 | drop_ratio: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# --- Logging ---\u001b[39;00m\n\u001b[1;32m     41\u001b[0m greedy_advantage, best_advantage, greedy_info_gain, _, a_loss \u001b[38;5;241m=\u001b[39m evaluate(data, loss_mask, sorl_config, model, search_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 81\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     adam(\n\u001b[1;32m    248\u001b[0m         params_with_grad,\n\u001b[1;32m    249\u001b[0m         grads,\n\u001b[1;32m    250\u001b[0m         exp_avgs,\n\u001b[1;32m    251\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    252\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    253\u001b[0m         state_steps,\n\u001b[1;32m    254\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    255\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    256\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    257\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    258\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    264\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    265\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    266\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    267\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    268\u001b[0m         decoupled_weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoupled_weight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:149\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:949\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 949\u001b[0m func(\n\u001b[1;32m    950\u001b[0m     params,\n\u001b[1;32m    951\u001b[0m     grads,\n\u001b[1;32m    952\u001b[0m     exp_avgs,\n\u001b[1;32m    953\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    954\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    955\u001b[0m     state_steps,\n\u001b[1;32m    956\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    957\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    958\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    959\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    960\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    961\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    962\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    963\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    964\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    965\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    966\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    967\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    968\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[1;32m    969\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:533\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    531\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    535\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.sorl import evaluate \n",
    "from src.sorl import compute_per_token_loss, compute_loss, sorl_search, SearchScheduler, GatedPhaseTransition\n",
    "\n",
    "# First, test out baseline performance, then test out SoRL performance etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "search_scheduler = SearchScheduler(sorl_config)\n",
    "gapt = GatedPhaseTransition(sorl_config.delta, sorl_config.tau, sorl_config.p_m, sorl_config.p_c)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "\n",
    "\n",
    "for i in range(sorl_config.train_iterations): # Run for 10 steps\n",
    "    # --- Scheduler Step ---\n",
    "    t_search, drop_ratio = search_scheduler.step()\n",
    "    sorl_config.max_t_search = 0\n",
    "    model.drop_ratio = 0.0\n",
    "\n",
    "    # --- Get data and perform SORL search ---\n",
    "    # (1). Apply loss mask (and change its shape with abs padding) || (2). Customize abs padding\n",
    "    data, loss_mask = train_loader.get_batch(sorl_config.train_batch_size)\n",
    "    with torch.no_grad():\n",
    "        search_data, switch_ratio = sorl_search(data, loss_mask, model, sorl_config)\n",
    "        \n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, search_data)\n",
    "    ssl_loss, abs_loss = compute_loss(search_data, model, ppt, loss_mask)\n",
    "    \n",
    "    total_loss = ssl_loss + abs_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    greedy_advantage, best_advantage, greedy_info_gain, _, a_loss = evaluate(data, loss_mask, sorl_config, model, search_n=1)\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.2f} (SSL: {ssl_loss.item():.3f}, Abs: {abs_loss.item():.2f}) | \"\n",
    "        f\"Advantage: {greedy_advantage:.1f}% | Info-Gain: {greedy_info_gain:.1f}% | Abs-Free-Loss: {a_loss:.3f} | \"\n",
    "        f\"t_search: {t_search} | \"\n",
    "        f\"drop_ratio: {model.drop_ratio:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb4ef57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235643564356436"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "583 * 128 / 80800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1f7f",
   "metadata": {},
   "source": [
    "$\\textbf{Question} 1$. What'd happen if the topological similarity approaches 1, and how can we make it so?\n",
    "$\\textbf{Question} 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b99adf",
   "metadata": {},
   "source": [
    "$\\textbf{Observation 1}$. 200 epochs is far from enough for minimid model to learn 2x2 multiplication. Think 2k epochs at least. We could also use a bigger batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0131bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Batches: 100%|██████████| 20/20 [00:12<00:00,  1.61it/s, Accuracy=15.50%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Summary ---\n",
      "Samples Evaluated: 200\n",
      "Correct Predictions: 31\n",
      "Accuracy: 15.50%\n",
      "Topological Similarity: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 15.5,\n",
       " 'correct': 31,\n",
       " 'total': 200,\n",
       " 'top_sim_score': 0.48200833816144695}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_multiply import evaluate_on_loader\n",
    "\n",
    "# --- Evaluation (Generate & Check Answer) ---- \n",
    "print(\"--- Running Evaluation ---\")\n",
    "evaluate_on_loader(model, tokenizer, val_loader, batch_size=10, K=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f285986",
   "metadata": {},
   "source": [
    "$\\textbf{Record 1.}$.  1.5 epochs (that's 80k * 1.5 = 120k data getting trained, with batch size of 128, roughly 1k iterations required) on 2x2 multiplication produces 66.5% accuracy on test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb81ac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f497bd9d8c864343939c2cb8f8d3cc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca4191d80854b28bd67456b558d47d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load the pretrained \"gpt2\" model (which is the 124M parameter version)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# You can print the model architecture to see its layers\n",
    "# print(model)\n",
    "\n",
    "# To get the total number of parameters, you can do the following:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# And to see how many of those are trainable:\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f2499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b4952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dbc2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 1 2 * 4 8 =\n",
      "Generated Response: <answer> 5 5 6 <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "Expected Answer:  5 7 6\n",
      "Generated Answer: 5 5 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('5 5 6', '5 7 6')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_multiply import evaluate_multiplication\n",
    "\n",
    "input_ids, loss_mask = val_loader.get_batch(1)\n",
    "prompt = tokenizer.decode(input_ids[0])\n",
    "evaluate_multiplication(model, tokenizer, prompt, K=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a518ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate train_loader data is correct \n",
    "from eval_multiply import _get_query_and_gt_ids\n",
    "\n",
    "input_ids, loss_mask = train_loader.get_batch(1)\n",
    "prompt = tokenizer.decode(input_ids[0])\n",
    "\n",
    "query_str = prompt.split('=')[0].strip() # execute this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38792e8",
   "metadata": {},
   "source": [
    "$\\textbf{Idea 3}$. How about using RL instead? \n",
    "\n",
    "$\\textbf{Idea 4}$. How about using SoRL instead? \n",
    "\n",
    "$\\textbf{Idea 5}$. How about using SoRL + RL instead? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8431a",
   "metadata": {},
   "source": [
    "Issue 1. Missing CoT generations. \n",
    "- $\\textit{Fix 1}$. Dynamically identify case requiring CoT and case that doesn't. \n",
    "\n",
    "Issue 2. I can't believe transformer can't learn multiplication (2 digits !?) -- Can we initialize a Qwen architecture and try on this again? Can we scale up the experiment script and run on GPU instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74b00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c6075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62cfe42d",
   "metadata": {},
   "source": [
    "$\\textbf{Issue 1}$. When including abstraction, the generated response don't 'stop' anymore. \n",
    "\n",
    "$\\textbf{Issue 2}$. Abstraction generation in 'train-time' mismatch with that of 'inference-time' (former is parallel search over query, latter is causal generation over answer).\n",
    "\n",
    "$\\textbf{Idea 1}$. We have mismatch between 'train-time' abstraction addition (which is on query), and 'test-time' abstraction addition (which is on answer). It's probably better to add abstraction on query, or prefix token sequence only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb46cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n"
     ]
    }
   ],
   "source": [
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "val_loader = MemLoader(args.val_data_path, device=args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_loader.tokenizer_path) # data is tokenized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = len(tokenizer)\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "# 2 layer 4 head\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = MiniMindForCausalLM(minimind_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1616a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 1.24\n",
      "Step 02 | Loss: 1.39\n",
      "Step 03 | Loss: 1.21\n",
      "Step 04 | Loss: 1.34\n",
      "Step 05 | Loss: 1.36\n",
      "Step 06 | Loss: 1.30\n",
      "Step 07 | Loss: 1.32\n",
      "Step 08 | Loss: 1.23\n",
      "Step 09 | Loss: 1.22\n",
      "Step 10 | Loss: 1.26\n",
      "Step 11 | Loss: 1.28\n",
      "Step 12 | Loss: 1.20\n",
      "Step 13 | Loss: 1.22\n",
      "Step 14 | Loss: 1.17\n",
      "Step 15 | Loss: 1.18\n",
      "Step 16 | Loss: 1.23\n",
      "Step 17 | Loss: 1.21\n",
      "Step 18 | Loss: 1.16\n",
      "Step 19 | Loss: 1.26\n",
      "Step 20 | Loss: 1.13\n",
      "Step 21 | Loss: 1.13\n",
      "Step 22 | Loss: 1.24\n",
      "Step 23 | Loss: 1.12\n",
      "Step 24 | Loss: 1.23\n",
      "Step 25 | Loss: 1.11\n",
      "Step 26 | Loss: 1.11\n",
      "Step 27 | Loss: 1.17\n",
      "Step 28 | Loss: 1.21\n",
      "Step 29 | Loss: 1.19\n",
      "Step 30 | Loss: 1.20\n",
      "Step 31 | Loss: 1.26\n",
      "Step 32 | Loss: 1.18\n",
      "Step 33 | Loss: 1.15\n",
      "Step 34 | Loss: 1.09\n",
      "Step 35 | Loss: 1.14\n",
      "Step 36 | Loss: 1.16\n",
      "Step 37 | Loss: 1.05\n",
      "Step 38 | Loss: 1.12\n",
      "Step 39 | Loss: 1.20\n",
      "Step 40 | Loss: 1.06\n",
      "Step 41 | Loss: 1.18\n",
      "Step 42 | Loss: 1.14\n",
      "Step 43 | Loss: 1.11\n",
      "Step 44 | Loss: 1.17\n",
      "Step 45 | Loss: 1.19\n",
      "Step 46 | Loss: 1.11\n",
      "Step 47 | Loss: 1.05\n",
      "Step 48 | Loss: 1.14\n",
      "Step 49 | Loss: 1.19\n",
      "Step 50 | Loss: 1.17\n",
      "Step 51 | Loss: 1.12\n",
      "Step 52 | Loss: 1.12\n",
      "Step 53 | Loss: 1.16\n",
      "Step 54 | Loss: 1.19\n",
      "Step 55 | Loss: 1.17\n",
      "Step 56 | Loss: 1.18\n",
      "Step 57 | Loss: 1.08\n",
      "Step 58 | Loss: 1.14\n",
      "Step 59 | Loss: 1.08\n",
      "Step 60 | Loss: 1.19\n",
      "Step 61 | Loss: 1.09\n",
      "Step 62 | Loss: 1.08\n",
      "Step 63 | Loss: 1.01\n",
      "Step 64 | Loss: 1.16\n",
      "Step 65 | Loss: 1.11\n",
      "Step 66 | Loss: 1.09\n",
      "Step 67 | Loss: 1.11\n",
      "Step 68 | Loss: 1.09\n",
      "Step 69 | Loss: 1.08\n",
      "Step 70 | Loss: 1.07\n",
      "Step 71 | Loss: 1.13\n",
      "Step 72 | Loss: 1.07\n",
      "Step 73 | Loss: 1.09\n",
      "Step 74 | Loss: 1.12\n",
      "Step 75 | Loss: 0.98\n",
      "Step 76 | Loss: 1.04\n",
      "Step 77 | Loss: 1.06\n",
      "Step 78 | Loss: 1.08\n",
      "Step 79 | Loss: 1.12\n",
      "Step 80 | Loss: 1.06\n",
      "Step 81 | Loss: 1.11\n",
      "Step 82 | Loss: 1.12\n",
      "Step 83 | Loss: 1.10\n",
      "Step 84 | Loss: 1.03\n",
      "Step 85 | Loss: 1.09\n",
      "Step 86 | Loss: 1.11\n",
      "Step 87 | Loss: 1.11\n",
      "Step 88 | Loss: 1.15\n",
      "Step 89 | Loss: 1.11\n",
      "Step 90 | Loss: 1.09\n",
      "Step 91 | Loss: 1.15\n",
      "Step 92 | Loss: 1.12\n",
      "Step 93 | Loss: 1.08\n",
      "Step 94 | Loss: 1.19\n",
      "Step 95 | Loss: 0.99\n",
      "Step 96 | Loss: 1.09\n",
      "Step 97 | Loss: 1.11\n",
      "Step 98 | Loss: 1.17\n",
      "Step 99 | Loss: 1.16\n",
      "Step 100 | Loss: 1.08\n",
      "Step 101 | Loss: 1.11\n",
      "Step 102 | Loss: 1.21\n",
      "Step 103 | Loss: 1.11\n",
      "Step 104 | Loss: 1.00\n",
      "Step 105 | Loss: 1.09\n",
      "Step 106 | Loss: 1.09\n",
      "Step 107 | Loss: 1.03\n",
      "Step 108 | Loss: 1.04\n",
      "Step 109 | Loss: 1.16\n",
      "Step 110 | Loss: 1.02\n",
      "Step 111 | Loss: 1.04\n",
      "Step 112 | Loss: 1.09\n",
      "Step 113 | Loss: 1.13\n",
      "Step 114 | Loss: 1.15\n",
      "Step 115 | Loss: 1.03\n",
      "Step 116 | Loss: 1.12\n",
      "Step 117 | Loss: 1.07\n",
      "Step 118 | Loss: 1.07\n",
      "Step 119 | Loss: 1.05\n",
      "Step 120 | Loss: 1.02\n",
      "Step 121 | Loss: 0.97\n",
      "Step 122 | Loss: 1.04\n",
      "Step 123 | Loss: 1.09\n",
      "Step 124 | Loss: 1.09\n",
      "Step 125 | Loss: 1.09\n",
      "Step 126 | Loss: 1.04\n",
      "Step 127 | Loss: 1.06\n",
      "Step 128 | Loss: 1.04\n",
      "Step 129 | Loss: 1.08\n",
      "Step 130 | Loss: 1.07\n",
      "Step 131 | Loss: 1.06\n",
      "Step 132 | Loss: 0.99\n",
      "Step 133 | Loss: 1.05\n",
      "Step 134 | Loss: 1.10\n",
      "Step 135 | Loss: 1.03\n",
      "Step 136 | Loss: 1.01\n",
      "Step 137 | Loss: 1.00\n",
      "Step 138 | Loss: 0.96\n",
      "Step 139 | Loss: 0.99\n",
      "Step 140 | Loss: 1.04\n",
      "Step 141 | Loss: 1.01\n",
      "Step 142 | Loss: 1.01\n",
      "Step 143 | Loss: 1.06\n",
      "Step 144 | Loss: 1.08\n",
      "Step 145 | Loss: 0.99\n",
      "Step 146 | Loss: 0.99\n",
      "Step 147 | Loss: 1.06\n",
      "Step 148 | Loss: 1.07\n",
      "Step 149 | Loss: 1.03\n",
      "Step 150 | Loss: 0.99\n",
      "Step 151 | Loss: 1.06\n",
      "Step 152 | Loss: 1.08\n",
      "Step 153 | Loss: 0.96\n",
      "Step 154 | Loss: 1.03\n",
      "Step 155 | Loss: 1.00\n",
      "Step 156 | Loss: 1.05\n",
      "Step 157 | Loss: 0.97\n",
      "Step 158 | Loss: 0.96\n",
      "Step 159 | Loss: 1.04\n",
      "Step 160 | Loss: 1.00\n",
      "Step 161 | Loss: 1.09\n",
      "Step 162 | Loss: 1.02\n",
      "Step 163 | Loss: 1.00\n",
      "Step 164 | Loss: 0.98\n",
      "Step 165 | Loss: 0.96\n",
      "Step 166 | Loss: 1.04\n",
      "Step 167 | Loss: 1.02\n",
      "Step 168 | Loss: 1.02\n",
      "Step 169 | Loss: 1.02\n",
      "Step 170 | Loss: 0.97\n",
      "Step 171 | Loss: 1.02\n",
      "Step 172 | Loss: 1.12\n",
      "Step 173 | Loss: 1.08\n",
      "Step 174 | Loss: 1.02\n",
      "Step 175 | Loss: 1.02\n",
      "Step 176 | Loss: 1.07\n",
      "Step 177 | Loss: 1.07\n",
      "Step 178 | Loss: 0.96\n",
      "Step 179 | Loss: 1.11\n",
      "Step 180 | Loss: 1.02\n",
      "Step 181 | Loss: 0.97\n",
      "Step 182 | Loss: 1.02\n",
      "Step 183 | Loss: 1.04\n",
      "Step 184 | Loss: 1.10\n",
      "Step 185 | Loss: 1.05\n",
      "Step 186 | Loss: 1.09\n",
      "Step 187 | Loss: 1.01\n",
      "Step 188 | Loss: 1.06\n",
      "Step 189 | Loss: 0.97\n",
      "Step 190 | Loss: 1.06\n",
      "Step 191 | Loss: 1.07\n",
      "Step 192 | Loss: 1.04\n",
      "Step 193 | Loss: 0.95\n",
      "Step 194 | Loss: 1.08\n",
      "Step 195 | Loss: 1.04\n",
      "Step 196 | Loss: 1.02\n",
      "Step 197 | Loss: 1.06\n",
      "Step 198 | Loss: 1.05\n",
      "Step 199 | Loss: 1.12\n",
      "Step 200 | Loss: 1.08\n"
     ]
    }
   ],
   "source": [
    "from src.sorl import evaluate \n",
    "from src.sorl import compute_per_token_loss, compute_loss, sorl_search, SearchScheduler, GatedPhaseTransition\n",
    "\n",
    "# First, test out baseline performance, then test out SoRL performance etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "\n",
    "for i in range(sorl_config.train_iterations): # Run for 10 steps\n",
    "\n",
    "    # --- Get data and perform SORL search ---\n",
    "    # (1). Apply loss mask (and change its shape with abs padding) || (2). Customize abs padding\n",
    "    data, loss_mask = train_loader.get_batch(sorl_config.train_batch_size)\n",
    "    # break\n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, data, tokenizer.pad_token_id)\n",
    "    ssl_loss = (ppt * loss_mask[:, 1:]).sum() / loss_mask[:, 1:].sum()\n",
    "    total_loss = ssl_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d044737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
