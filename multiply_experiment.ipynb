{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f514e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication Dataset (cot included/not, reverse/not)\n",
    "# - (no cot, no reverse) 11 x 12 = <answer> 132 \n",
    "# - (cot, no reverse) 11 x 12 = 12 + 120 = <answer> 132\n",
    "# - (no cot, reverse) 11 x 21 = <answer> 231 \n",
    "# - (cot, reverse) 11 x 21 = 21 + 021 = <answer> 231 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c60d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/digit_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3cec6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer/digit_tokenizer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The HuggingFace tokenizer does not have a `.filepath` attribute.\n",
    "# To get the path it was loaded from, use:\n",
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ea1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7693f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n",
      "Model initialized on cpu with 2.96M parameters.\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from dataset.base import MemLoader\n",
    "from src.sorl import SORLConfig\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration (Mimicking command-line args)\n",
    "# ==============================================================================\n",
    "args = SimpleNamespace(\n",
    "    # --- Paths ---\n",
    "    train_data_path=\"dataset/multiply/multiply_1x1_train.bin\",\n",
    "    val_data_path=\"dataset/multiply/multiply_1x1_val.bin\",\n",
    "    \n",
    "    # --- Model Config ---\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    abstract_vocab_sizes=\"8\",\n",
    "    \n",
    "    # --- Training Config ---\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    \n",
    "    # --- SORL Config ---\n",
    "    n_rollout=5,\n",
    "    temperature=1.0,\n",
    "    K=4,\n",
    "    denoise_steps=1,\n",
    "    max_t_search=0,\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False,\n",
    "    use_special_placeholders=False,\n",
    "    special_token_id=31,\n",
    "    abstract_budget=5,\n",
    "    temperature_flip=False,\n",
    "    \n",
    "    # --- Curriculum and Memory ---\n",
    "    curriculum_ratio=0.6, # looks redundant as of now, it (vaguely) violates the \"compositionality\" principle\n",
    "    train_iterations=200, # This will be used by the scheduler\n",
    "    use_fade_memory=False,\n",
    "    use_compression_mask=False, # <-- Set to True to test your new mask\n",
    "    compression_curriculum_ratio=0.25,\n",
    "    memory_span=20,\n",
    "    \n",
    "    # --- GAPT ---\n",
    "    default_phase=None, # Set to 1 or 2 to override, None to enable GAPT\n",
    "    delta=0.01,\n",
    "    tau=0.1,\n",
    "    p_m=10,\n",
    "    p_c=10\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "val_loader = MemLoader(args.val_data_path, device=args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_loader.tokenizer_path) # data is tokenized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = len(tokenizer)\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "# 2 layer 4 head\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=args.memory_span,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model initialized on {args.device} with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")\n",
    "\n",
    "# --- SORL Config and Schedulers ---\n",
    "sorl_config = SORLConfig(\n",
    "    n=args.n_rollout, \n",
    "    temperature=args.temperature, \n",
    "    K=args.K,\n",
    "    l=1, \n",
    "    steps=args.denoise_steps, \n",
    "    max_t_search=args.max_t_search,\n",
    "    use_rhythmic_placeholders=args.use_rhythmic_placeholders,\n",
    "    use_spike_placeholders=args.use_spike_placeholders,\n",
    "    use_special_placeholders=args.use_special_placeholders,\n",
    "    special_token_id=args.special_token_id,\n",
    "    abstract_budget=args.abstract_budget,\n",
    "    temperature_flip=args.temperature_flip,\n",
    "    curriculum_ratio=args.curriculum_ratio,\n",
    "    use_fade_memory=args.use_fade_memory,\n",
    "    use_compression_mask=args.use_compression_mask,\n",
    "    min_keep=args.memory_span, \n",
    "    max_seq_len=train_loader.max_length,\n",
    "    train_iterations=args.train_iterations, \n",
    "    train_batch_size=args.batch_size,\n",
    "    val_batch_size=args.batch_size,\n",
    "    max_length=train_loader.max_length,\n",
    "    default_phase=args.default_phase, \n",
    "    delta=args.delta, tau=args.tau,\n",
    "    p_m=args.p_m, p_c=args.p_c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff93c8f",
   "metadata": {},
   "source": [
    "$\\textbf{Bug 1}$. Multiplication with 2 digits are not trained perfectly, suggesting issue with data / training pipeline (SoRL)\n",
    "- loss is dropping (optimizer is fine, and loss function is consistent, potential data issue)\n",
    "- trained model do not generate an answer, suggesting issue with the loss function\n",
    "- $\\textbf{Fix 1}$. Found error in 'compute_loss' -- it's not compatible with .long typed loss_mask (ends up slicing the index-1 element repetitively instead of picking the index with True mask etc.)\n",
    "\n",
    "$\\textbf{Idea 1}$. We inherit the language-modeling vocabulary, which splits '1' and ' 1' as separate token, this will make the arithmetic rule MUCH more complex (combinatorial space is much larger, therefore Radamacher complexity grows, which increases the generalization error etc.) --> It's worth trying a small, concise tokenizer. \n",
    "\n",
    "$\\textbf{Idea 2}$. Topological similarity can potentially explain the curve / trend (generalization error v.s. BPE tokenization size, multiplication specific) (currently our hypothesis), and it can be a general metric adaptable for non-multiplication task easily as well. SoRL should be adopted from basic vocabulary, in order to maximize the topological similarity and show its efficiency here.\n",
    "\n",
    "$\\textbf{Progress 1}$. Build a collection of tokenizer, from digit tokenizer all the way to increasing sized BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dce59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 3.44 (SSL: 3.438, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.8% | Abs-Free-Loss: 1.197 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 02 | Loss: 2.48 (SSL: 2.484, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 1.056 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 03 | Loss: 2.11 (SSL: 2.112, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.976 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 04 | Loss: 1.87 (SSL: 1.866, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.861 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 05 | Loss: 1.69 (SSL: 1.689, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.805 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 06 | Loss: 1.38 (SSL: 1.377, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.607 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 07 | Loss: 1.24 (SSL: 1.236, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.563 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 08 | Loss: 1.30 (SSL: 1.300, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.628 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 09 | Loss: 1.09 (SSL: 1.089, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.488 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 10 | Loss: 1.03 (SSL: 1.031, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.470 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 11 | Loss: 1.00 (SSL: 0.997, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.487 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 12 | Loss: 1.02 (SSL: 1.020, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.499 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 13 | Loss: 0.87 (SSL: 0.869, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.432 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 14 | Loss: 0.77 (SSL: 0.770, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.369 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 15 | Loss: 0.80 (SSL: 0.801, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.383 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 16 | Loss: 0.76 (SSL: 0.762, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.373 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 17 | Loss: 0.80 (SSL: 0.801, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.400 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 18 | Loss: 0.77 (SSL: 0.766, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.378 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 19 | Loss: 0.71 (SSL: 0.711, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.338 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 20 | Loss: 0.81 (SSL: 0.805, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.387 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 21 | Loss: 0.70 (SSL: 0.698, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.325 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 22 | Loss: 0.82 (SSL: 0.819, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.375 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 23 | Loss: 0.70 (SSL: 0.697, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.350 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 24 | Loss: 0.64 (SSL: 0.640, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.312 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 25 | Loss: 0.58 (SSL: 0.583, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.282 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 26 | Loss: 0.62 (SSL: 0.625, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.290 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 27 | Loss: 0.65 (SSL: 0.654, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.317 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 28 | Loss: 0.65 (SSL: 0.649, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.303 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 29 | Loss: 0.55 (SSL: 0.545, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.255 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 30 | Loss: 0.52 (SSL: 0.519, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.248 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 31 | Loss: 0.59 (SSL: 0.592, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.295 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 32 | Loss: 0.58 (SSL: 0.575, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.261 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 33 | Loss: 0.51 (SSL: 0.511, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.240 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 34 | Loss: 0.42 (SSL: 0.424, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.215 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 35 | Loss: 0.50 (SSL: 0.496, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.217 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 36 | Loss: 0.46 (SSL: 0.458, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.222 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 37 | Loss: 0.41 (SSL: 0.415, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.187 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 38 | Loss: 0.28 (SSL: 0.280, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.136 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 39 | Loss: 0.29 (SSL: 0.286, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -111.5% | Abs-Free-Loss: 0.127 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 40 | Loss: 0.30 (SSL: 0.303, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.147 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 41 | Loss: 0.33 (SSL: 0.327, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.128 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 42 | Loss: 0.31 (SSL: 0.305, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.141 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 43 | Loss: 0.21 (SSL: 0.213, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.089 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 44 | Loss: 0.25 (SSL: 0.253, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.104 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 45 | Loss: 0.15 (SSL: 0.151, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -109.6% | Abs-Free-Loss: 0.073 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 46 | Loss: 0.17 (SSL: 0.169, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.077 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 47 | Loss: 0.18 (SSL: 0.176, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.084 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 48 | Loss: 0.15 (SSL: 0.152, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -109.6% | Abs-Free-Loss: 0.061 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 49 | Loss: 0.16 (SSL: 0.157, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.064 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 50 | Loss: 0.13 (SSL: 0.131, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.059 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 51 | Loss: 0.10 (SSL: 0.098, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.044 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 52 | Loss: 0.07 (SSL: 0.074, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.035 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 53 | Loss: 0.11 (SSL: 0.114, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.051 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 54 | Loss: 0.10 (SSL: 0.104, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.043 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 55 | Loss: 0.08 (SSL: 0.077, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.032 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 56 | Loss: 0.07 (SSL: 0.068, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.030 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 57 | Loss: 0.05 (SSL: 0.051, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.023 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 58 | Loss: 0.07 (SSL: 0.071, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.033 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 59 | Loss: 0.07 (SSL: 0.072, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.031 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 60 | Loss: 0.04 (SSL: 0.041, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.018 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 61 | Loss: 0.03 (SSL: 0.027, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.014 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 62 | Loss: 0.03 (SSL: 0.031, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.016 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 63 | Loss: 0.02 (SSL: 0.024, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.011 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 64 | Loss: 0.04 (SSL: 0.040, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.015 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 65 | Loss: 0.02 (SSL: 0.020, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.009 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 66 | Loss: 0.02 (SSL: 0.021, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.010 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 67 | Loss: 0.02 (SSL: 0.017, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.009 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 68 | Loss: 0.03 (SSL: 0.026, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.010 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 69 | Loss: 0.02 (SSL: 0.018, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.008 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 70 | Loss: 0.02 (SSL: 0.019, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.010 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 71 | Loss: 0.04 (SSL: 0.035, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.012 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 72 | Loss: 0.01 (SSL: 0.012, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.006 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 73 | Loss: 0.04 (SSL: 0.038, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.013 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 74 | Loss: 0.02 (SSL: 0.016, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.008 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 75 | Loss: 0.02 (SSL: 0.016, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.007 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 76 | Loss: 0.02 (SSL: 0.017, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.009 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 77 | Loss: 0.01 (SSL: 0.010, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.005 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 78 | Loss: 0.02 (SSL: 0.019, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -107.8% | Abs-Free-Loss: 0.007 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 79 | Loss: 0.01 (SSL: 0.013, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.006 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 80 | Loss: 0.01 (SSL: 0.009, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 81 | Loss: 0.01 (SSL: 0.008, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -107.8% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 82 | Loss: 0.01 (SSL: 0.011, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.005 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 83 | Loss: 0.01 (SSL: 0.009, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.005 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 84 | Loss: 0.01 (SSL: 0.009, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.005 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 85 | Loss: 0.01 (SSL: 0.010, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.005 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 86 | Loss: 0.01 (SSL: 0.009, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 87 | Loss: 0.01 (SSL: 0.009, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 88 | Loss: 0.01 (SSL: 0.012, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 89 | Loss: 0.01 (SSL: 0.007, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 90 | Loss: 0.01 (SSL: 0.007, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 91 | Loss: 0.01 (SSL: 0.007, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 92 | Loss: 0.01 (SSL: 0.007, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 93 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 94 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 95 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 96 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 97 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 98 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 99 | Loss: 0.01 (SSL: 0.013, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 100 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 101 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -85.9% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 102 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 103 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 104 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 105 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 106 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 107 | Loss: 0.05 (SSL: 0.053, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 108 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 109 | Loss: 0.00 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 110 | Loss: 0.00 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 111 | Loss: 0.01 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 112 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -109.6% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 113 | Loss: 0.01 (SSL: 0.007, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -107.8% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 114 | Loss: 0.01 (SSL: 0.007, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.8% | Abs-Free-Loss: 0.004 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 115 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 116 | Loss: 0.01 (SSL: 0.013, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 117 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 118 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 119 | Loss: 0.00 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 120 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 121 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -87.8% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 122 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 123 | Loss: 0.00 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 124 | Loss: 0.01 (SSL: 0.008, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 125 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 126 | Loss: 0.01 (SSL: 0.006, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.003 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 127 | Loss: 0.00 (SSL: 0.005, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 128 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -111.5% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 129 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 130 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -111.5% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 131 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 132 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 133 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 134 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 135 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 136 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 137 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 138 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 139 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 140 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 141 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 142 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 143 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 144 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 145 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 146 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 147 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 148 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 149 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 150 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 151 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 152 | Loss: 0.00 (SSL: 0.004, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 153 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 154 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 155 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 156 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 157 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 158 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -109.6% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 159 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -106.0% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 160 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 161 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 162 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 163 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 164 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 165 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 166 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 167 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 168 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -107.8% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 169 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 170 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -107.8% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 171 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 172 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 173 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 174 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 175 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -84.1% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 176 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 177 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: -0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 178 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 179 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 180 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 181 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -89.6% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 182 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 183 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -109.6% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 184 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -107.8% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 185 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -95.1% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 186 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 187 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 188 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 189 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -93.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 190 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 191 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 192 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 193 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 194 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -91.4% | Abs-Free-Loss: 0.002 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 195 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 196 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -100.5% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 197 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -98.7% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 198 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -96.9% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 199 | Loss: 0.00 (SSL: 0.002, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -104.2% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 200 | Loss: 0.00 (SSL: 0.003, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -102.3% | Abs-Free-Loss: 0.001 | t_search: 0 | drop_ratio: 0.00\n"
     ]
    }
   ],
   "source": [
    "from src.sorl import evaluate \n",
    "from src.sorl import compute_per_token_loss, compute_loss, sorl_search, SearchScheduler, GatedPhaseTransition\n",
    "\n",
    "# First, test out baseline performance, then test out SoRL performance etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "search_scheduler = SearchScheduler(sorl_config)\n",
    "gapt = GatedPhaseTransition(sorl_config.delta, sorl_config.tau, sorl_config.p_m, sorl_config.p_c)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "for i in range(sorl_config.train_iterations): # Run for 10 steps\n",
    "    # --- Scheduler Step ---\n",
    "    t_search, drop_ratio = search_scheduler.step()\n",
    "    sorl_config.max_t_search = 0\n",
    "    model.drop_ratio = 0.0\n",
    "\n",
    "    # --- Get data and perform SORL search ---\n",
    "    # (1). Apply loss mask (and change its shape with abs padding) || (2). Customize abs padding\n",
    "    data, loss_mask = train_loader.get_batch(sorl_config.train_batch_size)\n",
    "    with torch.no_grad():\n",
    "        search_data, switch_ratio = sorl_search(data, loss_mask, model, sorl_config)\n",
    "        \n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, search_data)\n",
    "    ssl_loss, abs_loss = compute_loss(search_data, model, ppt, loss_mask)\n",
    "    \n",
    "    total_loss = ssl_loss + abs_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    greedy_advantage, best_advantage, greedy_info_gain, _, a_loss = evaluate(data, loss_mask, sorl_config, model, search_n=1)\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.2f} (SSL: {ssl_loss.item():.3f}, Abs: {abs_loss.item():.2f}) | \"\n",
    "        f\"Advantage: {greedy_advantage:.1f}% | Info-Gain: {greedy_info_gain:.1f}% | Abs-Free-Loss: {a_loss:.3f} | \"\n",
    "        f\"t_search: {t_search} | \"\n",
    "        f\"drop_ratio: {model.drop_ratio:.2f}\"\n",
    "    )\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0131bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Computing Topological Similarity of Number Embeddings (10-99) ---\n",
      "Topological Similarity: 0.42\n",
      "--- Running Evaluation ---\n",
      "Query: 8 * 4 = <answer>\n",
      "Generated Response: 3 2 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "Expected Answer:  3 2\n",
      "Generated Answer: 3 2\n"
     ]
    }
   ],
   "source": [
    "from eval_multiply import compute_topological_similarity, evaluate_multiplication\n",
    "\n",
    "# --- Topological Similarity Metric ---\n",
    "print(\"--- Computing Topological Similarity of Number Embeddings (10-99) ---\")\n",
    "number_strings = [str(i) for i in range(10, 100)]\n",
    "correlation = compute_topological_similarity(model, tokenizer, number_strings)\n",
    "print(f\"Topological Similarity: {correlation:.2f}\")\n",
    "\n",
    "\n",
    "# --- Evaluation (Generate & Check Answer) ---- \n",
    "input_ids, _ = val_loader.get_batch(2)\n",
    "test_prompt = tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Clean up padding for the prompt\n",
    "test_prompt = test_prompt.replace(tokenizer.pad_token, '').strip()\n",
    "\n",
    "print(\"--- Running Evaluation ---\")\n",
    "evaluate_multiplication(model, tokenizer, test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808b77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
