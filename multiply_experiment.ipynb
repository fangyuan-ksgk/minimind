{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f514e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication Dataset (cot included/not, reverse/not)\n",
    "# - (no cot, no reverse) 11 x 12 = <answer> 132 \n",
    "# - (cot, no reverse) 11 x 12 = 12 + 120 = <answer> 132\n",
    "# - (no cot, reverse) 11 x 21 = <answer> 231 \n",
    "# - (cot, reverse) 11 x 21 = 21 + 021 = <answer> 231 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c60d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/digit_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3cec6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer/digit_tokenizer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The HuggingFace tokenizer does not have a `.filepath` attribute.\n",
    "# To get the path it was loaded from, use:\n",
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7693f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n",
      "Model initialized on cpu with 4.29M parameters.\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from dataset.base import MemLoader\n",
    "from src.sorl import SORLConfig\n",
    "import os \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration (Mimicking command-line args)\n",
    "# ==============================================================================\n",
    "args = SimpleNamespace(\n",
    "    # --- Paths ---\n",
    "    train_data_path=\"dataset/multiply/multiply_2x2_train.bin\",\n",
    "    val_data_path=\"dataset/multiply/multiply_2x2_val.bin\",\n",
    "    \n",
    "    # --- Model Config ---\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=6,\n",
    "    abstract_vocab_sizes=\"8\",\n",
    "    \n",
    "    # --- Training Config ---\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    \n",
    "    # --- SORL Config ---\n",
    "    n_rollout=5,\n",
    "    temperature=1.0,\n",
    "    K=4,\n",
    "    denoise_steps=1,\n",
    "    max_t_search=0,\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False,\n",
    "    use_special_placeholders=False,\n",
    "    special_token_id=31,\n",
    "    abstract_budget=5,\n",
    "    temperature_flip=False,\n",
    "    \n",
    "    # --- Curriculum and Memory ---\n",
    "    curriculum_ratio=0.6, # looks redundant as of now, it (vaguely) violates the \"compositionality\" principle\n",
    "    train_iterations=200, # This will be used by the scheduler\n",
    "    use_fade_memory=False,\n",
    "    use_compression_mask=False, # <-- Set to True to test your new mask\n",
    "    compression_curriculum_ratio=0.25,\n",
    "    memory_span=128,\n",
    "    \n",
    "    # --- GAPT ---\n",
    "    default_phase=None, # Set to 1 or 2 to override, None to enable GAPT\n",
    "    delta=0.01,\n",
    "    tau=0.1,\n",
    "    p_m=10,\n",
    "    p_c=10\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "val_loader = MemLoader(args.val_data_path, device=args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_loader.tokenizer_path) # data is tokenized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = len(tokenizer)\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "# 2 layer 4 head\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=args.memory_span,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model initialized on {args.device} with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")\n",
    "\n",
    "# --- SORL Config and Schedulers ---\n",
    "sorl_config = SORLConfig(\n",
    "    n=args.n_rollout, \n",
    "    temperature=args.temperature, \n",
    "    K=args.K,\n",
    "    l=1, \n",
    "    steps=args.denoise_steps, \n",
    "    max_t_search=args.max_t_search,\n",
    "    use_rhythmic_placeholders=args.use_rhythmic_placeholders,\n",
    "    use_spike_placeholders=args.use_spike_placeholders,\n",
    "    use_special_placeholders=args.use_special_placeholders,\n",
    "    special_token_id=args.special_token_id,\n",
    "    abstract_budget=args.abstract_budget,\n",
    "    temperature_flip=args.temperature_flip,\n",
    "    curriculum_ratio=args.curriculum_ratio,\n",
    "    use_fade_memory=args.use_fade_memory,\n",
    "    use_compression_mask=args.use_compression_mask,\n",
    "    min_keep=args.memory_span, \n",
    "    max_seq_len=train_loader.max_length,\n",
    "    train_iterations=args.train_iterations, \n",
    "    train_batch_size=args.batch_size,\n",
    "    val_batch_size=args.batch_size,\n",
    "    max_length=train_loader.max_length,\n",
    "    default_phase=args.default_phase, \n",
    "    delta=args.delta, tau=args.tau,\n",
    "    p_m=args.p_m, p_c=args.p_c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff93c8f",
   "metadata": {},
   "source": [
    "$\\textbf{Bug 1}$. Multiplication with 2 digits are not trained perfectly, suggesting issue with data / training pipeline (SoRL)\n",
    "- loss is dropping (optimizer is fine, and loss function is consistent, potential data issue)\n",
    "- trained model do not generate an answer, suggesting issue with the loss function\n",
    "- $\\textbf{Fix 1}$. Found error in 'compute_loss' -- it's not compatible with .long typed loss_mask (ends up slicing the index-1 element repetitively instead of picking the index with True mask etc.)\n",
    "\n",
    "$\\textbf{Idea 1}$. We inherit the language-modeling vocabulary, which splits '1' and ' 1' as separate token, this will make the arithmetic rule MUCH more complex (combinatorial space is much larger, therefore Radamacher complexity grows, which increases the generalization error etc.) --> It's worth trying a small, concise tokenizer. \n",
    "\n",
    "$\\textbf{Idea 2}$. Topological similarity can potentially explain the curve / trend (generalization error v.s. BPE tokenization size, multiplication specific) (currently our hypothesis), and it can be a general metric adaptable for non-multiplication task easily as well. SoRL should be adopted from basic vocabulary, in order to maximize the topological similarity and show its efficiency here.\n",
    "\n",
    "$\\textbf{Progress 1}$. Build a collection of tokenizer, from digit tokenizer all the way to increasing sized BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76dce59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 1.35 (SSL: 1.345, Abs: 0.00) | Advantage: 58.1% | Info-Gain: 47.9% | Abs-Free-Loss: 3.973 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 02 | Loss: 1.41 (SSL: 1.412, Abs: 0.00) | Advantage: 63.8% | Info-Gain: 45.4% | Abs-Free-Loss: 3.947 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 03 | Loss: 1.49 (SSL: 1.488, Abs: 0.00) | Advantage: 63.2% | Info-Gain: 42.9% | Abs-Free-Loss: 3.876 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 04 | Loss: 1.38 (SSL: 1.373, Abs: 0.00) | Advantage: 62.6% | Info-Gain: 43.9% | Abs-Free-Loss: 3.804 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 05 | Loss: 1.35 (SSL: 1.343, Abs: 0.00) | Advantage: 63.5% | Info-Gain: 44.5% | Abs-Free-Loss: 3.825 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 06 | Loss: 1.39 (SSL: 1.387, Abs: 0.00) | Advantage: 65.3% | Info-Gain: 44.6% | Abs-Free-Loss: 3.910 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 07 | Loss: 1.37 (SSL: 1.365, Abs: 0.00) | Advantage: 61.5% | Info-Gain: 47.2% | Abs-Free-Loss: 4.022 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 08 | Loss: 1.34 (SSL: 1.340, Abs: 0.00) | Advantage: 66.7% | Info-Gain: 47.7% | Abs-Free-Loss: 3.985 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 09 | Loss: 1.47 (SSL: 1.467, Abs: 0.00) | Advantage: 61.7% | Info-Gain: 43.3% | Abs-Free-Loss: 3.831 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 10 | Loss: 1.35 (SSL: 1.348, Abs: 0.00) | Advantage: 63.2% | Info-Gain: 44.5% | Abs-Free-Loss: 3.863 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 11 | Loss: 1.33 (SSL: 1.324, Abs: 0.00) | Advantage: 64.1% | Info-Gain: 41.8% | Abs-Free-Loss: 3.772 | t_search: 0 | drop_ratio: 0.50\n",
      "Step 12 | Loss: 1.40 (SSL: 1.400, Abs: 0.00) | Advantage: 63.6% | Info-Gain: 46.1% | Abs-Free-Loss: 3.929 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 13 | Loss: 1.34 (SSL: 1.333, Abs: 0.00) | Advantage: 69.1% | Info-Gain: 45.6% | Abs-Free-Loss: 3.875 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 14 | Loss: 1.38 (SSL: 1.375, Abs: 0.00) | Advantage: 60.7% | Info-Gain: 46.8% | Abs-Free-Loss: 3.933 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 15 | Loss: 1.39 (SSL: 1.383, Abs: 0.00) | Advantage: 63.2% | Info-Gain: 44.3% | Abs-Free-Loss: 3.873 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 16 | Loss: 1.39 (SSL: 1.383, Abs: 0.00) | Advantage: 66.8% | Info-Gain: 45.3% | Abs-Free-Loss: 3.914 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 17 | Loss: 1.32 (SSL: 1.322, Abs: 0.00) | Advantage: 64.3% | Info-Gain: 48.2% | Abs-Free-Loss: 3.981 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 18 | Loss: 1.37 (SSL: 1.369, Abs: 0.00) | Advantage: 60.8% | Info-Gain: 44.0% | Abs-Free-Loss: 3.839 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 19 | Loss: 1.42 (SSL: 1.413, Abs: 0.00) | Advantage: 62.6% | Info-Gain: 45.2% | Abs-Free-Loss: 3.959 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 20 | Loss: 1.43 (SSL: 1.431, Abs: 0.00) | Advantage: 62.4% | Info-Gain: 47.1% | Abs-Free-Loss: 4.027 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 21 | Loss: 1.30 (SSL: 1.297, Abs: 0.00) | Advantage: 60.0% | Info-Gain: 43.5% | Abs-Free-Loss: 3.822 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 22 | Loss: 1.37 (SSL: 1.370, Abs: 0.00) | Advantage: 64.1% | Info-Gain: 42.7% | Abs-Free-Loss: 3.809 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 23 | Loss: 1.36 (SSL: 1.361, Abs: 0.00) | Advantage: 62.2% | Info-Gain: 47.4% | Abs-Free-Loss: 3.980 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 24 | Loss: 1.35 (SSL: 1.347, Abs: 0.00) | Advantage: 60.1% | Info-Gain: 45.6% | Abs-Free-Loss: 3.989 | t_search: 1 | drop_ratio: 0.50\n",
      "Step 25 | Loss: 1.32 (SSL: 1.314, Abs: 0.00) | Advantage: 61.6% | Info-Gain: 46.8% | Abs-Free-Loss: 3.972 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 26 | Loss: 1.38 (SSL: 1.382, Abs: 0.00) | Advantage: 63.2% | Info-Gain: 46.3% | Abs-Free-Loss: 4.083 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 27 | Loss: 1.33 (SSL: 1.327, Abs: 0.00) | Advantage: 64.8% | Info-Gain: 47.4% | Abs-Free-Loss: 4.055 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 28 | Loss: 1.31 (SSL: 1.307, Abs: 0.00) | Advantage: 67.4% | Info-Gain: 47.2% | Abs-Free-Loss: 4.032 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 29 | Loss: 1.33 (SSL: 1.329, Abs: 0.00) | Advantage: 66.1% | Info-Gain: 45.0% | Abs-Free-Loss: 4.020 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 30 | Loss: 1.32 (SSL: 1.322, Abs: 0.00) | Advantage: 67.1% | Info-Gain: 47.5% | Abs-Free-Loss: 4.017 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 31 | Loss: 1.36 (SSL: 1.360, Abs: 0.00) | Advantage: 64.8% | Info-Gain: 44.3% | Abs-Free-Loss: 3.941 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 32 | Loss: 1.31 (SSL: 1.306, Abs: 0.00) | Advantage: 62.8% | Info-Gain: 47.7% | Abs-Free-Loss: 4.131 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 33 | Loss: 1.35 (SSL: 1.346, Abs: 0.00) | Advantage: 68.3% | Info-Gain: 48.6% | Abs-Free-Loss: 4.119 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 34 | Loss: 1.30 (SSL: 1.299, Abs: 0.00) | Advantage: 63.2% | Info-Gain: 49.7% | Abs-Free-Loss: 4.155 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 35 | Loss: 1.36 (SSL: 1.357, Abs: 0.00) | Advantage: 64.4% | Info-Gain: 46.7% | Abs-Free-Loss: 4.080 | t_search: 2 | drop_ratio: 0.50\n",
      "Step 36 | Loss: 1.28 (SSL: 1.277, Abs: 0.00) | Advantage: 65.9% | Info-Gain: 48.8% | Abs-Free-Loss: 4.064 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 37 | Loss: 1.31 (SSL: 1.312, Abs: 0.00) | Advantage: 62.2% | Info-Gain: 45.8% | Abs-Free-Loss: 3.940 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 38 | Loss: 1.28 (SSL: 1.278, Abs: 0.00) | Advantage: 64.4% | Info-Gain: 45.2% | Abs-Free-Loss: 3.948 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 39 | Loss: 1.43 (SSL: 1.432, Abs: 0.00) | Advantage: 68.2% | Info-Gain: 47.6% | Abs-Free-Loss: 4.107 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 40 | Loss: 1.35 (SSL: 1.348, Abs: 0.00) | Advantage: 66.7% | Info-Gain: 47.5% | Abs-Free-Loss: 4.060 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 41 | Loss: 1.35 (SSL: 1.352, Abs: 0.00) | Advantage: 65.5% | Info-Gain: 45.4% | Abs-Free-Loss: 4.028 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 42 | Loss: 1.37 (SSL: 1.367, Abs: 0.00) | Advantage: 66.6% | Info-Gain: 44.4% | Abs-Free-Loss: 3.961 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 43 | Loss: 1.33 (SSL: 1.325, Abs: 0.00) | Advantage: 60.5% | Info-Gain: 44.9% | Abs-Free-Loss: 3.963 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 44 | Loss: 1.33 (SSL: 1.329, Abs: 0.00) | Advantage: 62.5% | Info-Gain: 47.1% | Abs-Free-Loss: 4.114 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 45 | Loss: 1.31 (SSL: 1.304, Abs: 0.00) | Advantage: 66.9% | Info-Gain: 46.9% | Abs-Free-Loss: 4.047 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 46 | Loss: 1.30 (SSL: 1.300, Abs: 0.00) | Advantage: 65.0% | Info-Gain: 49.0% | Abs-Free-Loss: 4.095 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 47 | Loss: 1.27 (SSL: 1.266, Abs: 0.00) | Advantage: 65.4% | Info-Gain: 49.3% | Abs-Free-Loss: 4.161 | t_search: 3 | drop_ratio: 0.50\n",
      "Step 48 | Loss: 1.30 (SSL: 1.295, Abs: 0.00) | Advantage: 66.4% | Info-Gain: 48.0% | Abs-Free-Loss: 4.068 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 49 | Loss: 1.31 (SSL: 1.308, Abs: 0.00) | Advantage: 63.7% | Info-Gain: 49.3% | Abs-Free-Loss: 4.219 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 50 | Loss: 1.33 (SSL: 1.330, Abs: 0.00) | Advantage: 66.5% | Info-Gain: 46.2% | Abs-Free-Loss: 4.087 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 51 | Loss: 1.36 (SSL: 1.361, Abs: 0.00) | Advantage: 66.3% | Info-Gain: 46.3% | Abs-Free-Loss: 4.137 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 52 | Loss: 1.28 (SSL: 1.281, Abs: 0.00) | Advantage: 65.3% | Info-Gain: 47.8% | Abs-Free-Loss: 4.121 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 53 | Loss: 1.34 (SSL: 1.339, Abs: 0.00) | Advantage: 59.3% | Info-Gain: 47.1% | Abs-Free-Loss: 4.061 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 54 | Loss: 1.32 (SSL: 1.316, Abs: 0.00) | Advantage: 64.9% | Info-Gain: 48.3% | Abs-Free-Loss: 4.087 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 55 | Loss: 1.21 (SSL: 1.208, Abs: 0.00) | Advantage: 66.7% | Info-Gain: 47.6% | Abs-Free-Loss: 4.042 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 56 | Loss: 1.38 (SSL: 1.374, Abs: 0.00) | Advantage: 62.9% | Info-Gain: 43.1% | Abs-Free-Loss: 3.907 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 57 | Loss: 1.29 (SSL: 1.291, Abs: 0.00) | Advantage: 64.6% | Info-Gain: 50.7% | Abs-Free-Loss: 4.191 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 58 | Loss: 1.25 (SSL: 1.250, Abs: 0.00) | Advantage: 69.6% | Info-Gain: 49.8% | Abs-Free-Loss: 4.146 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 59 | Loss: 1.29 (SSL: 1.287, Abs: 0.00) | Advantage: 63.2% | Info-Gain: 46.5% | Abs-Free-Loss: 4.046 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 60 | Loss: 1.30 (SSL: 1.301, Abs: 0.00) | Advantage: 63.7% | Info-Gain: 46.7% | Abs-Free-Loss: 4.043 | t_search: 4 | drop_ratio: 0.50\n",
      "Step 61 | Loss: 1.31 (SSL: 1.312, Abs: 0.00) | Advantage: 67.7% | Info-Gain: 47.4% | Abs-Free-Loss: 4.149 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 62 | Loss: 1.38 (SSL: 1.373, Abs: 0.00) | Advantage: 68.2% | Info-Gain: 49.6% | Abs-Free-Loss: 4.195 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 63 | Loss: 1.30 (SSL: 1.298, Abs: 0.00) | Advantage: 62.6% | Info-Gain: 49.0% | Abs-Free-Loss: 4.206 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 64 | Loss: 1.25 (SSL: 1.251, Abs: 0.00) | Advantage: 69.7% | Info-Gain: 48.2% | Abs-Free-Loss: 4.064 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 65 | Loss: 1.26 (SSL: 1.259, Abs: 0.00) | Advantage: 69.0% | Info-Gain: 50.8% | Abs-Free-Loss: 4.193 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 66 | Loss: 1.26 (SSL: 1.258, Abs: 0.00) | Advantage: 64.1% | Info-Gain: 50.2% | Abs-Free-Loss: 4.174 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 67 | Loss: 1.28 (SSL: 1.280, Abs: 0.00) | Advantage: 68.5% | Info-Gain: 47.2% | Abs-Free-Loss: 4.052 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 68 | Loss: 1.28 (SSL: 1.280, Abs: 0.00) | Advantage: 64.7% | Info-Gain: 48.4% | Abs-Free-Loss: 4.103 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 69 | Loss: 1.34 (SSL: 1.335, Abs: 0.00) | Advantage: 64.3% | Info-Gain: 47.8% | Abs-Free-Loss: 4.146 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 70 | Loss: 1.25 (SSL: 1.243, Abs: 0.00) | Advantage: 65.8% | Info-Gain: 47.0% | Abs-Free-Loss: 3.996 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 71 | Loss: 1.30 (SSL: 1.301, Abs: 0.00) | Advantage: 67.2% | Info-Gain: 49.3% | Abs-Free-Loss: 4.132 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 72 | Loss: 1.30 (SSL: 1.295, Abs: 0.00) | Advantage: 69.2% | Info-Gain: 50.0% | Abs-Free-Loss: 4.165 | t_search: 5 | drop_ratio: 0.50\n",
      "Step 73 | Loss: 1.39 (SSL: 1.384, Abs: 0.00) | Advantage: 61.0% | Info-Gain: 46.5% | Abs-Free-Loss: 4.020 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 74 | Loss: 1.34 (SSL: 1.336, Abs: 0.00) | Advantage: 63.8% | Info-Gain: 47.4% | Abs-Free-Loss: 4.050 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 75 | Loss: 1.29 (SSL: 1.287, Abs: 0.00) | Advantage: 66.8% | Info-Gain: 47.2% | Abs-Free-Loss: 4.058 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 76 | Loss: 1.30 (SSL: 1.297, Abs: 0.00) | Advantage: 62.8% | Info-Gain: 46.9% | Abs-Free-Loss: 3.984 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 77 | Loss: 1.22 (SSL: 1.221, Abs: 0.00) | Advantage: 65.8% | Info-Gain: 47.4% | Abs-Free-Loss: 4.007 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 78 | Loss: 1.26 (SSL: 1.254, Abs: 0.00) | Advantage: 64.1% | Info-Gain: 44.4% | Abs-Free-Loss: 3.864 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 79 | Loss: 1.38 (SSL: 1.375, Abs: 0.00) | Advantage: 67.1% | Info-Gain: 45.1% | Abs-Free-Loss: 3.870 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 80 | Loss: 1.34 (SSL: 1.341, Abs: 0.00) | Advantage: 62.1% | Info-Gain: 48.5% | Abs-Free-Loss: 4.081 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 81 | Loss: 1.32 (SSL: 1.315, Abs: 0.00) | Advantage: 58.2% | Info-Gain: 46.7% | Abs-Free-Loss: 3.912 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 82 | Loss: 1.24 (SSL: 1.238, Abs: 0.00) | Advantage: 64.1% | Info-Gain: 49.2% | Abs-Free-Loss: 4.096 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 83 | Loss: 1.34 (SSL: 1.337, Abs: 0.00) | Advantage: 61.9% | Info-Gain: 47.8% | Abs-Free-Loss: 4.023 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 84 | Loss: 1.24 (SSL: 1.238, Abs: 0.00) | Advantage: 65.6% | Info-Gain: 47.5% | Abs-Free-Loss: 3.986 | t_search: 6 | drop_ratio: 0.50\n",
      "Step 85 | Loss: 1.29 (SSL: 1.289, Abs: 0.00) | Advantage: 65.7% | Info-Gain: 48.6% | Abs-Free-Loss: 4.000 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 86 | Loss: 1.27 (SSL: 1.269, Abs: 0.00) | Advantage: 64.7% | Info-Gain: 48.5% | Abs-Free-Loss: 3.993 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 87 | Loss: 1.28 (SSL: 1.283, Abs: 0.00) | Advantage: 57.8% | Info-Gain: 45.6% | Abs-Free-Loss: 3.929 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 88 | Loss: 1.34 (SSL: 1.335, Abs: 0.00) | Advantage: 64.6% | Info-Gain: 47.9% | Abs-Free-Loss: 4.082 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 89 | Loss: 1.27 (SSL: 1.271, Abs: 0.00) | Advantage: 64.8% | Info-Gain: 47.2% | Abs-Free-Loss: 3.980 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 90 | Loss: 1.25 (SSL: 1.249, Abs: 0.00) | Advantage: 63.5% | Info-Gain: 47.3% | Abs-Free-Loss: 3.960 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 91 | Loss: 1.28 (SSL: 1.276, Abs: 0.00) | Advantage: 66.3% | Info-Gain: 48.3% | Abs-Free-Loss: 4.000 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 92 | Loss: 1.27 (SSL: 1.268, Abs: 0.00) | Advantage: 66.5% | Info-Gain: 48.0% | Abs-Free-Loss: 4.025 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 93 | Loss: 1.29 (SSL: 1.283, Abs: 0.00) | Advantage: 65.9% | Info-Gain: 46.4% | Abs-Free-Loss: 3.941 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 94 | Loss: 1.28 (SSL: 1.281, Abs: 0.00) | Advantage: 63.6% | Info-Gain: 48.0% | Abs-Free-Loss: 4.009 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 95 | Loss: 1.23 (SSL: 1.224, Abs: 0.00) | Advantage: 69.6% | Info-Gain: 50.0% | Abs-Free-Loss: 4.098 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 96 | Loss: 1.19 (SSL: 1.186, Abs: 0.00) | Advantage: 67.3% | Info-Gain: 49.5% | Abs-Free-Loss: 4.050 | t_search: 7 | drop_ratio: 0.50\n",
      "Step 97 | Loss: 1.30 (SSL: 1.295, Abs: 0.00) | Advantage: 65.2% | Info-Gain: 48.1% | Abs-Free-Loss: 4.046 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 98 | Loss: 1.23 (SSL: 1.228, Abs: 0.00) | Advantage: 66.2% | Info-Gain: 48.9% | Abs-Free-Loss: 3.988 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 99 | Loss: 1.31 (SSL: 1.310, Abs: 0.00) | Advantage: 62.6% | Info-Gain: 45.1% | Abs-Free-Loss: 3.950 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 100 | Loss: 1.14 (SSL: 1.138, Abs: 0.00) | Advantage: 66.8% | Info-Gain: 49.0% | Abs-Free-Loss: 4.034 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 101 | Loss: 1.27 (SSL: 1.268, Abs: 0.00) | Advantage: 69.8% | Info-Gain: 48.6% | Abs-Free-Loss: 4.039 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 102 | Loss: 1.30 (SSL: 1.299, Abs: 0.00) | Advantage: 68.3% | Info-Gain: 49.1% | Abs-Free-Loss: 4.117 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 103 | Loss: 1.32 (SSL: 1.323, Abs: 0.00) | Advantage: 65.9% | Info-Gain: 48.4% | Abs-Free-Loss: 4.065 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 104 | Loss: 1.24 (SSL: 1.242, Abs: 0.00) | Advantage: 67.3% | Info-Gain: 49.3% | Abs-Free-Loss: 4.140 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 105 | Loss: 1.22 (SSL: 1.217, Abs: 0.00) | Advantage: 63.7% | Info-Gain: 49.0% | Abs-Free-Loss: 4.089 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 106 | Loss: 1.29 (SSL: 1.290, Abs: 0.00) | Advantage: 67.8% | Info-Gain: 48.3% | Abs-Free-Loss: 4.063 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 107 | Loss: 1.19 (SSL: 1.192, Abs: 0.00) | Advantage: 61.9% | Info-Gain: 48.9% | Abs-Free-Loss: 4.075 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 108 | Loss: 1.21 (SSL: 1.210, Abs: 0.00) | Advantage: 68.0% | Info-Gain: 49.9% | Abs-Free-Loss: 4.158 | t_search: 8 | drop_ratio: 0.50\n",
      "Step 109 | Loss: 1.29 (SSL: 1.288, Abs: 0.00) | Advantage: 65.6% | Info-Gain: 47.5% | Abs-Free-Loss: 4.049 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 110 | Loss: 1.24 (SSL: 1.242, Abs: 0.00) | Advantage: 64.9% | Info-Gain: 49.0% | Abs-Free-Loss: 4.015 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 111 | Loss: 1.31 (SSL: 1.310, Abs: 0.00) | Advantage: 65.7% | Info-Gain: 48.5% | Abs-Free-Loss: 4.053 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 112 | Loss: 1.27 (SSL: 1.264, Abs: 0.00) | Advantage: 67.4% | Info-Gain: 47.7% | Abs-Free-Loss: 3.993 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 113 | Loss: 1.24 (SSL: 1.235, Abs: 0.00) | Advantage: 61.9% | Info-Gain: 49.6% | Abs-Free-Loss: 4.127 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 114 | Loss: 1.19 (SSL: 1.188, Abs: 0.00) | Advantage: 67.0% | Info-Gain: 51.7% | Abs-Free-Loss: 4.137 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 115 | Loss: 1.23 (SSL: 1.232, Abs: 0.00) | Advantage: 66.1% | Info-Gain: 48.4% | Abs-Free-Loss: 3.991 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 116 | Loss: 1.20 (SSL: 1.197, Abs: 0.00) | Advantage: 67.3% | Info-Gain: 48.9% | Abs-Free-Loss: 3.937 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 117 | Loss: 1.16 (SSL: 1.154, Abs: 0.00) | Advantage: 72.1% | Info-Gain: 51.8% | Abs-Free-Loss: 4.142 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 118 | Loss: 1.24 (SSL: 1.240, Abs: 0.00) | Advantage: 63.8% | Info-Gain: 51.7% | Abs-Free-Loss: 4.181 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 119 | Loss: 1.27 (SSL: 1.266, Abs: 0.00) | Advantage: 66.5% | Info-Gain: 50.5% | Abs-Free-Loss: 4.115 | t_search: 9 | drop_ratio: 0.50\n",
      "Step 120 | Loss: 1.22 (SSL: 1.219, Abs: 0.00) | Advantage: 68.2% | Info-Gain: 50.8% | Abs-Free-Loss: 4.117 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 121 | Loss: 1.26 (SSL: 1.257, Abs: 0.00) | Advantage: 64.4% | Info-Gain: 49.3% | Abs-Free-Loss: 4.054 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 122 | Loss: 1.20 (SSL: 1.194, Abs: 0.00) | Advantage: 67.3% | Info-Gain: 48.7% | Abs-Free-Loss: 3.954 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 123 | Loss: 1.21 (SSL: 1.213, Abs: 0.00) | Advantage: 70.0% | Info-Gain: 50.7% | Abs-Free-Loss: 4.090 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 124 | Loss: 1.17 (SSL: 1.170, Abs: 0.00) | Advantage: 69.9% | Info-Gain: 52.2% | Abs-Free-Loss: 4.140 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 125 | Loss: 1.22 (SSL: 1.217, Abs: 0.00) | Advantage: 66.4% | Info-Gain: 48.5% | Abs-Free-Loss: 4.083 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 126 | Loss: 1.26 (SSL: 1.263, Abs: 0.00) | Advantage: 64.4% | Info-Gain: 46.4% | Abs-Free-Loss: 4.001 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 127 | Loss: 1.25 (SSL: 1.245, Abs: 0.00) | Advantage: 64.3% | Info-Gain: 49.2% | Abs-Free-Loss: 4.065 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 128 | Loss: 1.23 (SSL: 1.228, Abs: 0.00) | Advantage: 68.4% | Info-Gain: 51.8% | Abs-Free-Loss: 4.209 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 129 | Loss: 1.27 (SSL: 1.263, Abs: 0.00) | Advantage: 66.0% | Info-Gain: 48.2% | Abs-Free-Loss: 4.064 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 130 | Loss: 1.15 (SSL: 1.145, Abs: 0.00) | Advantage: 66.9% | Info-Gain: 49.1% | Abs-Free-Loss: 4.046 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 131 | Loss: 1.24 (SSL: 1.234, Abs: 0.00) | Advantage: 66.8% | Info-Gain: 47.3% | Abs-Free-Loss: 3.927 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 132 | Loss: 1.14 (SSL: 1.140, Abs: 0.00) | Advantage: 66.6% | Info-Gain: 49.9% | Abs-Free-Loss: 4.095 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 133 | Loss: 1.25 (SSL: 1.247, Abs: 0.00) | Advantage: 66.6% | Info-Gain: 46.7% | Abs-Free-Loss: 4.021 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 134 | Loss: 1.29 (SSL: 1.288, Abs: 0.00) | Advantage: 65.1% | Info-Gain: 48.9% | Abs-Free-Loss: 4.064 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 135 | Loss: 1.18 (SSL: 1.180, Abs: 0.00) | Advantage: 62.3% | Info-Gain: 50.1% | Abs-Free-Loss: 4.063 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 136 | Loss: 1.25 (SSL: 1.252, Abs: 0.00) | Advantage: 67.3% | Info-Gain: 48.8% | Abs-Free-Loss: 4.080 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 137 | Loss: 1.21 (SSL: 1.211, Abs: 0.00) | Advantage: 65.7% | Info-Gain: 50.3% | Abs-Free-Loss: 4.079 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 138 | Loss: 1.19 (SSL: 1.190, Abs: 0.00) | Advantage: 62.1% | Info-Gain: 51.2% | Abs-Free-Loss: 4.169 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 139 | Loss: 1.17 (SSL: 1.164, Abs: 0.00) | Advantage: 66.6% | Info-Gain: 49.4% | Abs-Free-Loss: 4.024 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 140 | Loss: 1.18 (SSL: 1.176, Abs: 0.00) | Advantage: 66.4% | Info-Gain: 52.4% | Abs-Free-Loss: 4.113 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 141 | Loss: 1.13 (SSL: 1.124, Abs: 0.00) | Advantage: 66.3% | Info-Gain: 52.3% | Abs-Free-Loss: 4.086 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 142 | Loss: 1.15 (SSL: 1.146, Abs: 0.00) | Advantage: 69.6% | Info-Gain: 51.1% | Abs-Free-Loss: 3.992 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 143 | Loss: 1.18 (SSL: 1.179, Abs: 0.00) | Advantage: 61.1% | Info-Gain: 48.5% | Abs-Free-Loss: 3.977 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 144 | Loss: 1.24 (SSL: 1.239, Abs: 0.00) | Advantage: 67.4% | Info-Gain: 50.6% | Abs-Free-Loss: 4.074 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 145 | Loss: 1.15 (SSL: 1.149, Abs: 0.00) | Advantage: 70.5% | Info-Gain: 49.7% | Abs-Free-Loss: 3.925 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 146 | Loss: 1.26 (SSL: 1.257, Abs: 0.00) | Advantage: 66.4% | Info-Gain: 52.5% | Abs-Free-Loss: 4.170 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 147 | Loss: 1.22 (SSL: 1.220, Abs: 0.00) | Advantage: 67.6% | Info-Gain: 48.4% | Abs-Free-Loss: 4.005 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 148 | Loss: 1.18 (SSL: 1.175, Abs: 0.00) | Advantage: 67.6% | Info-Gain: 52.3% | Abs-Free-Loss: 4.145 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 149 | Loss: 1.20 (SSL: 1.202, Abs: 0.00) | Advantage: 67.0% | Info-Gain: 48.3% | Abs-Free-Loss: 3.923 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 150 | Loss: 1.17 (SSL: 1.170, Abs: 0.00) | Advantage: 68.9% | Info-Gain: 53.0% | Abs-Free-Loss: 4.131 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 151 | Loss: 1.19 (SSL: 1.188, Abs: 0.00) | Advantage: 67.0% | Info-Gain: 54.7% | Abs-Free-Loss: 4.210 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 152 | Loss: 1.19 (SSL: 1.189, Abs: 0.00) | Advantage: 65.8% | Info-Gain: 51.5% | Abs-Free-Loss: 4.096 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 153 | Loss: 1.16 (SSL: 1.157, Abs: 0.00) | Advantage: 69.4% | Info-Gain: 51.5% | Abs-Free-Loss: 4.005 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 154 | Loss: 1.15 (SSL: 1.145, Abs: 0.00) | Advantage: 65.7% | Info-Gain: 52.5% | Abs-Free-Loss: 4.091 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 155 | Loss: 1.18 (SSL: 1.180, Abs: 0.00) | Advantage: 65.8% | Info-Gain: 50.5% | Abs-Free-Loss: 4.090 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 156 | Loss: 1.14 (SSL: 1.138, Abs: 0.00) | Advantage: 62.0% | Info-Gain: 49.8% | Abs-Free-Loss: 4.014 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 157 | Loss: 1.19 (SSL: 1.189, Abs: 0.00) | Advantage: 67.4% | Info-Gain: 54.0% | Abs-Free-Loss: 4.214 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 158 | Loss: 1.17 (SSL: 1.171, Abs: 0.00) | Advantage: 65.8% | Info-Gain: 52.8% | Abs-Free-Loss: 4.139 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 159 | Loss: 1.18 (SSL: 1.177, Abs: 0.00) | Advantage: 66.9% | Info-Gain: 50.3% | Abs-Free-Loss: 4.087 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 160 | Loss: 1.18 (SSL: 1.178, Abs: 0.00) | Advantage: 69.2% | Info-Gain: 50.6% | Abs-Free-Loss: 4.036 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 161 | Loss: 1.17 (SSL: 1.170, Abs: 0.00) | Advantage: 63.8% | Info-Gain: 52.1% | Abs-Free-Loss: 4.240 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 162 | Loss: 1.12 (SSL: 1.120, Abs: 0.00) | Advantage: 63.6% | Info-Gain: 51.5% | Abs-Free-Loss: 4.210 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 163 | Loss: 1.15 (SSL: 1.143, Abs: 0.00) | Advantage: 67.1% | Info-Gain: 50.1% | Abs-Free-Loss: 4.045 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 164 | Loss: 1.13 (SSL: 1.129, Abs: 0.00) | Advantage: 69.3% | Info-Gain: 52.9% | Abs-Free-Loss: 4.150 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 165 | Loss: 1.04 (SSL: 1.040, Abs: 0.00) | Advantage: 67.6% | Info-Gain: 50.8% | Abs-Free-Loss: 4.109 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 166 | Loss: 1.10 (SSL: 1.097, Abs: 0.00) | Advantage: 67.3% | Info-Gain: 51.5% | Abs-Free-Loss: 4.077 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 167 | Loss: 1.16 (SSL: 1.162, Abs: 0.00) | Advantage: 68.8% | Info-Gain: 49.4% | Abs-Free-Loss: 3.996 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 168 | Loss: 1.13 (SSL: 1.133, Abs: 0.00) | Advantage: 67.9% | Info-Gain: 52.1% | Abs-Free-Loss: 4.137 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 169 | Loss: 1.13 (SSL: 1.125, Abs: 0.00) | Advantage: 66.8% | Info-Gain: 51.7% | Abs-Free-Loss: 4.151 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 170 | Loss: 1.17 (SSL: 1.166, Abs: 0.00) | Advantage: 68.1% | Info-Gain: 49.1% | Abs-Free-Loss: 3.962 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 171 | Loss: 1.20 (SSL: 1.196, Abs: 0.00) | Advantage: 66.5% | Info-Gain: 52.8% | Abs-Free-Loss: 4.159 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 172 | Loss: 1.26 (SSL: 1.258, Abs: 0.00) | Advantage: 67.7% | Info-Gain: 51.7% | Abs-Free-Loss: 4.126 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 173 | Loss: 1.16 (SSL: 1.162, Abs: 0.00) | Advantage: 66.1% | Info-Gain: 52.0% | Abs-Free-Loss: 4.206 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 174 | Loss: 1.14 (SSL: 1.139, Abs: 0.00) | Advantage: 67.6% | Info-Gain: 49.3% | Abs-Free-Loss: 4.089 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 175 | Loss: 1.17 (SSL: 1.167, Abs: 0.00) | Advantage: 63.0% | Info-Gain: 51.5% | Abs-Free-Loss: 4.208 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 176 | Loss: 1.15 (SSL: 1.153, Abs: 0.00) | Advantage: 67.2% | Info-Gain: 51.5% | Abs-Free-Loss: 4.147 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 177 | Loss: 1.16 (SSL: 1.158, Abs: 0.00) | Advantage: 68.0% | Info-Gain: 51.7% | Abs-Free-Loss: 4.141 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 178 | Loss: 1.15 (SSL: 1.144, Abs: 0.00) | Advantage: 68.1% | Info-Gain: 48.9% | Abs-Free-Loss: 4.038 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 179 | Loss: 1.23 (SSL: 1.229, Abs: 0.00) | Advantage: 63.7% | Info-Gain: 51.5% | Abs-Free-Loss: 4.145 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 180 | Loss: 1.07 (SSL: 1.071, Abs: 0.00) | Advantage: 68.4% | Info-Gain: 51.6% | Abs-Free-Loss: 4.069 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 181 | Loss: 1.15 (SSL: 1.151, Abs: 0.00) | Advantage: 68.6% | Info-Gain: 49.8% | Abs-Free-Loss: 4.073 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 182 | Loss: 1.11 (SSL: 1.108, Abs: 0.00) | Advantage: 67.7% | Info-Gain: 53.1% | Abs-Free-Loss: 4.225 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 183 | Loss: 1.17 (SSL: 1.166, Abs: 0.00) | Advantage: 70.7% | Info-Gain: 54.0% | Abs-Free-Loss: 4.214 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 184 | Loss: 1.12 (SSL: 1.119, Abs: 0.00) | Advantage: 68.1% | Info-Gain: 52.0% | Abs-Free-Loss: 4.156 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 185 | Loss: 1.16 (SSL: 1.159, Abs: 0.00) | Advantage: 69.4% | Info-Gain: 53.2% | Abs-Free-Loss: 4.234 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 186 | Loss: 1.19 (SSL: 1.192, Abs: 0.00) | Advantage: 66.2% | Info-Gain: 54.9% | Abs-Free-Loss: 4.252 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 187 | Loss: 1.12 (SSL: 1.117, Abs: 0.00) | Advantage: 68.9% | Info-Gain: 52.2% | Abs-Free-Loss: 4.170 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 188 | Loss: 1.14 (SSL: 1.136, Abs: 0.00) | Advantage: 69.8% | Info-Gain: 52.3% | Abs-Free-Loss: 4.203 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 189 | Loss: 1.12 (SSL: 1.121, Abs: 0.00) | Advantage: 69.6% | Info-Gain: 53.8% | Abs-Free-Loss: 4.137 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 190 | Loss: 1.21 (SSL: 1.212, Abs: 0.00) | Advantage: 69.2% | Info-Gain: 52.8% | Abs-Free-Loss: 4.279 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 191 | Loss: 1.06 (SSL: 1.063, Abs: 0.00) | Advantage: 65.7% | Info-Gain: 51.6% | Abs-Free-Loss: 4.137 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 192 | Loss: 1.01 (SSL: 1.011, Abs: 0.00) | Advantage: 73.1% | Info-Gain: 55.6% | Abs-Free-Loss: 4.219 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 193 | Loss: 1.18 (SSL: 1.178, Abs: 0.00) | Advantage: 69.8% | Info-Gain: 50.7% | Abs-Free-Loss: 4.174 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 194 | Loss: 1.10 (SSL: 1.095, Abs: 0.01) | Advantage: 66.9% | Info-Gain: 52.9% | Abs-Free-Loss: 4.214 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 195 | Loss: 1.12 (SSL: 1.121, Abs: 0.00) | Advantage: 72.4% | Info-Gain: 53.8% | Abs-Free-Loss: 4.236 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 196 | Loss: 1.10 (SSL: 1.097, Abs: 0.00) | Advantage: 69.3% | Info-Gain: 53.9% | Abs-Free-Loss: 4.280 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 197 | Loss: 1.12 (SSL: 1.115, Abs: 0.00) | Advantage: 72.6% | Info-Gain: 54.4% | Abs-Free-Loss: 4.217 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 198 | Loss: 1.11 (SSL: 1.105, Abs: 0.00) | Advantage: 70.0% | Info-Gain: 54.4% | Abs-Free-Loss: 4.222 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 199 | Loss: 1.16 (SSL: 1.154, Abs: 0.00) | Advantage: 69.6% | Info-Gain: 52.3% | Abs-Free-Loss: 4.108 | t_search: 10 | drop_ratio: 0.50\n",
      "Step 200 | Loss: 1.12 (SSL: 1.123, Abs: 0.00) | Advantage: 67.0% | Info-Gain: 52.4% | Abs-Free-Loss: 4.033 | t_search: 10 | drop_ratio: 0.50\n"
     ]
    }
   ],
   "source": [
    "from src.sorl import evaluate \n",
    "from src.sorl import compute_per_token_loss, compute_loss, sorl_search, SearchScheduler, GatedPhaseTransition\n",
    "\n",
    "# First, test out baseline performance, then test out SoRL performance etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "search_scheduler = SearchScheduler(sorl_config)\n",
    "gapt = GatedPhaseTransition(sorl_config.delta, sorl_config.tau, sorl_config.p_m, sorl_config.p_c)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "\n",
    "\n",
    "for i in range(sorl_config.train_iterations): # Run for 10 steps\n",
    "    # --- Scheduler Step ---\n",
    "    t_search, drop_ratio = search_scheduler.step()\n",
    "    sorl_config.max_t_search = 10\n",
    "    model.drop_ratio = 0.5\n",
    "\n",
    "    # --- Get data and perform SORL search ---\n",
    "    # (1). Apply loss mask (and change its shape with abs padding) || (2). Customize abs padding\n",
    "    data, loss_mask = train_loader.get_batch(sorl_config.train_batch_size)\n",
    "    with torch.no_grad():\n",
    "        search_data, switch_ratio = sorl_search(data, loss_mask, model, sorl_config)\n",
    "        \n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, search_data)\n",
    "    ssl_loss, abs_loss = compute_loss(search_data, model, ppt, loss_mask)\n",
    "    \n",
    "    total_loss = ssl_loss + abs_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    greedy_advantage, best_advantage, greedy_info_gain, _, a_loss = evaluate(data, loss_mask, sorl_config, model, search_n=1)\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.2f} (SSL: {ssl_loss.item():.3f}, Abs: {abs_loss.item():.2f}) | \"\n",
    "        f\"Advantage: {greedy_advantage:.1f}% | Info-Gain: {greedy_info_gain:.1f}% | Abs-Free-Loss: {a_loss:.3f} | \"\n",
    "        f\"t_search: {t_search} | \"\n",
    "        f\"drop_ratio: {model.drop_ratio:.2f}\"\n",
    "    )\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1f7f",
   "metadata": {},
   "source": [
    "$\\textbf{Question} 1$. What'd happen if the topological similarity approaches 1, and how can we make it so?\n",
    "$\\textbf{Question} 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0131bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Computing Topological Similarity of Number Embeddings (10-99) ---\n",
      "Topological Similarity: 0.49\n",
      "--- Running Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Batches: 100%|██████████| 20/20 [00:03<00:00,  5.43it/s, Accuracy=0.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Summary ---\n",
      "Samples Evaluated: 200\n",
      "Correct Predictions: 0\n",
      "Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0, 'correct': 0, 'total': 200}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_multiply import compute_topological_similarity, evaluate_on_loader\n",
    "\n",
    "# --- Topological Similarity Metric ---\n",
    "print(\"--- Computing Topological Similarity of Number Embeddings (10-99) ---\")\n",
    "number_strings = [str(i) for i in range(10, 100)]\n",
    "correlation = compute_topological_similarity(model, tokenizer, number_strings)\n",
    "print(f\"Topological Similarity: {correlation:.2f}\")\n",
    "\n",
    "\n",
    "# --- Evaluation (Generate & Check Answer) ---- \n",
    "input_ids, _ = val_loader.get_batch(2)\n",
    "test_prompt = tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Clean up padding for the prompt\n",
    "test_prompt = test_prompt.replace(tokenizer.pad_token, '').strip()\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"--- Running Evaluation ---\")\n",
    "evaluate_on_loader(model, tokenizer, train_loader, batch_size=10, K=sorl_config.K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38792e8",
   "metadata": {},
   "source": [
    "$\\textbf{Idea 3}$. How about using RL instead? \n",
    "\n",
    "$\\textbf{Idea 4}$. How about using SoRL instead? \n",
    "\n",
    "$\\textbf{Idea 5}$. How about using SoRL + RL instead? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "140e1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 6 0 * 8 9 =\n",
      "Generated Response: 5 5 9 9 9 9 9 9 9 9\n",
      "Expected Answer:  5 3 4 0\n",
      "Generated Answer: 5 5 9 9 9 9 9 9 9 9\n",
      "Query: 2 5 * 5 1 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 2 7 5\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 6 9 * 3 8 =\n",
      "Generated Response: 5 9 9 9 9 9 9 9 9 9\n",
      "Expected Answer:  2 6 2 2\n",
      "Generated Answer: 5 9 9 9 9 9 9 9 9 9\n",
      "Query: 6 6 * 1 6 =\n",
      "Generated Response: 5 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 0 5 6\n",
      "Generated Answer: 5 2 2 2 2 2 2 2 2 2\n",
      "Query: 8 1 * 7 0 =\n",
      "Generated Response: 7 7 7 7 7 7 7 7 9 9\n",
      "Expected Answer:  5 6 7 0\n",
      "Generated Answer: 7 7 7 7 7 7 7 7 9 9\n",
      "Query: 1 1 * 1 0 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 <eos>\n",
      "Expected Answer:  1 1 0\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2\n",
      "Query: 5 8 * 1 2 =\n",
      "Generated Response: 3 3 3 2 2 2 2 2 2 2\n",
      "Expected Answer:  6 9 6\n",
      "Generated Answer: 3 3 3 2 2 2 2 2 2 2\n",
      "Query: 7 5 * 8 2 =\n",
      "Generated Response: 7 7 7 7 9 9 9 9 9 9\n",
      "Expected Answer:  6 1 5 0\n",
      "Generated Answer: 7 7 7 7 9 9 9 9 9 9\n",
      "Query: 9 6 * 7 1 =\n",
      "Generated Response: 7 7 7 7 9 9 9 9 9 9\n",
      "Expected Answer:  6 8 1 6\n",
      "Generated Answer: 7 7 7 7 9 9 9 9 9 9\n",
      "Query: 6 0 * 3 2 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 9 2 0\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 4 1 * 2 3 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  9 4 3\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 8 4 * 8 3 =\n",
      "Generated Response: 7 7 7 7 7 7 7 7 9 9\n",
      "Expected Answer:  6 9 7 2\n",
      "Generated Answer: 7 7 7 7 7 7 7 7 9 9\n",
      "Query: 2 3 * 7 1 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 6 3 3\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 4 0 * 3 4 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 3 6 0\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 2 8 * 8 9 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  2 4 9 2\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 1 4 * 6 8 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  9 5 2\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 7 1 * 3 5 =\n",
      "Generated Response: 5 9 9 9 9 9 9 9 9 9\n",
      "Expected Answer:  2 4 8 5\n",
      "Generated Answer: 5 9 9 9 9 9 9 9 9 9\n",
      "Query: 8 1 * 7 6 =\n",
      "Generated Response: 7 7 7 7 7 7 7 7 7 7\n",
      "Expected Answer:  6 1 5 6\n",
      "Generated Answer: 7 7 7 7 7 7 7 7 7 7\n",
      "Query: 4 3 * 1 7 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  7 3 1\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 5 8 * 8 0 =\n",
      "Generated Response: 4 4 9 9 9 9 9 9 9 9\n",
      "Expected Answer:  4 6 4 0\n",
      "Generated Answer: 4 4 9 9 9 9 9 9 9 9\n",
      "Query: 7 1 * 9 1 =\n",
      "Generated Response: 7 7 9 9 9 9 9 9 9 9\n",
      "Expected Answer:  6 4 6 1\n",
      "Generated Answer: 7 7 9 9 9 9 9 9 9 9\n",
      "Query: 5 5 * 5 4 =\n",
      "Generated Response: 3 3 3 3 3 3 3 3 3 3\n",
      "Expected Answer:  2 9 7 0\n",
      "Generated Answer: 3 3 3 3 3 3 3 3 3 3\n",
      "Query: 1 4 * 7 6 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 0 6 4\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 4 2 * 5 9 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  2 4 7 8\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 5 5 * 1 3 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  7 1 5\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 9 9 * 2 9 =\n",
      "Generated Response: 7 7 7 7 8 8 8 8 8 8\n",
      "Expected Answer:  2 8 7 1\n",
      "Generated Answer: 7 7 7 7 8 8 8 8 8 8\n",
      "Query: 2 2 * 1 7 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  3 7 4\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 7 1 * 6 6 =\n",
      "Generated Response: 5 7 7 7 7 9 9 9 9 9\n",
      "Expected Answer:  4 6 8 6\n",
      "Generated Answer: 5 7 7 7 7 9 9 9 9 9\n",
      "Query: 1 1 * 6 4 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  7 0 4\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 7 1 * 3 9 =\n",
      "Generated Response: 5 9 9 9 9 9 9 9 9 9\n",
      "Expected Answer:  2 7 6 9\n",
      "Generated Answer: 5 9 9 9 9 9 9 9 9 9\n",
      "Query: 2 7 * 5 2 =\n",
      "Generated Response: 2 2 2 2 2 2 2 2 2 2\n",
      "Expected Answer:  1 4 0 4\n",
      "Generated Answer: 2 2 2 2 2 2 2 2 2 2\n",
      "Query: 4 1 * 5 8 =\n",
      "Generated Response: 3 3 3 3 3 3 3 3 3 3\n",
      "Expected Answer:  2 3 7 8\n",
      "Generated Answer: 3 3 3 3 3 3 3 3 3 3\n"
     ]
    }
   ],
   "source": [
    "from eval_multiply import evaluate_multiplication\n",
    "\n",
    "for d in data: \n",
    "    test_prompt = tokenizer.decode(d)\n",
    "    evaluate_multiplication(model, tokenizer, test_prompt, K=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c41bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 4 1 * 5 8 =\n",
      "Generated Response: 3 3 3 3 3 3 3 3 3 3\n",
      "Expected Answer:  2 3 7 8\n",
      "Generated Answer: 3 3 3 3 3 3 3 3 3 3\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = test_prompt \n",
    "\n",
    "from eval_multiply import _extract_answer_from_ids\n",
    "from model.model_sorl import infer_level\n",
    "\n",
    "answer_token_id = tokenizer.encode('<answer>', add_special_tokens=False)[0]\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "answer_indices = torch.where(input_ids == answer_token_id)\n",
    "\n",
    "answer_idx = answer_indices[1][0]\n",
    "query_ids = input_ids[:, :answer_idx + 1]\n",
    "ground_truth_ids = input_ids[:, answer_idx + 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Issue 1. keep generating abstract tokens\n",
    "    output = model.generate(\n",
    "                query_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.0,\n",
    "                force_abstraction_every_n=None\n",
    "    )\n",
    "\n",
    "# Use utility function for parsing\n",
    "level = infer_level(output, model.vocab_sizes, tokenizer.pad_token_id)\n",
    "traj = output[level == 0].reshape(output.shape[0], -1)\n",
    "generated_ids = traj[:, query_ids.shape[1]:]\n",
    "generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "generated_answer = _extract_answer_from_ids(generated_ids[0], tokenizer)\n",
    "ground_truth_answer = _extract_answer_from_ids(ground_truth_ids[0], tokenizer)\n",
    "\n",
    "print(f\"Query: {tokenizer.decode(query_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Generated Response: {generated_response.strip()}\")\n",
    "print(f\"Expected Answer:  {ground_truth_answer}\")\n",
    "print(f\"Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39a1664d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  4, 13,  8, 11, 14,  2,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de559ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  3, 13, 11, 21, 12, 14,  2,  8, 21,  6,  7,  3, 19],\n",
       "        [ 5,  8, 13,  8, 21,  4, 14,  2,  4, 21,  5, 10,  8, 19],\n",
       "        [ 9, 12, 13,  6, 21, 11, 14,  2,  5, 21,  9,  5,  5, 19],\n",
       "        [ 9,  9, 13,  4, 21,  9, 14,  2,  4, 21,  3,  8,  9, 19],\n",
       "        [11,  4, 13, 10, 21,  3, 14,  2,  8, 21,  9, 10,  3, 19],\n",
       "        [ 4,  4, 13,  4, 21,  3, 14,  2,  4, 21,  4,  3, 19,  1],\n",
       "        [ 8, 11, 13,  4, 21,  5, 14,  2,  9, 21, 12,  9, 19,  1],\n",
       "        [10,  8, 13, 11, 21,  5, 14,  2,  9, 21,  4,  8,  3, 19],\n",
       "        [12,  9, 13, 10, 21,  4, 14,  2,  9, 21, 11,  4,  9, 19],\n",
       "        [ 9,  3, 13,  6, 21,  5, 14,  2,  4, 21, 12,  5,  3, 19],\n",
       "        [ 7,  4, 13,  5, 21,  6, 14,  2, 12, 21,  7,  6, 19,  1],\n",
       "        [11,  7, 13, 11, 21,  6, 14,  2,  9, 21, 12, 10,  5, 19],\n",
       "        [ 5,  6, 13, 10, 21,  4, 14,  2,  4, 21,  9,  6,  6, 19],\n",
       "        [ 7,  3, 13,  6, 21,  7, 14,  2,  4, 21,  6,  9,  3, 19],\n",
       "        [ 5, 11, 13, 11, 21, 12, 14,  2,  5, 21,  7, 12,  5, 19],\n",
       "        [ 4,  7, 13,  9, 21, 11, 14,  2, 12, 21,  8,  5, 19,  1],\n",
       "        [10,  4, 13,  6, 21,  8, 14,  2,  5, 21,  7, 11,  8, 19],\n",
       "        [11,  4, 13, 10, 21,  9, 14,  2,  9, 21,  4,  8,  9, 19],\n",
       "        [ 7,  6, 13,  4, 21, 10, 14,  2, 10, 21,  6,  4, 19,  1],\n",
       "        [ 8, 11, 13, 11, 21,  3, 14,  2,  7, 21,  9,  7,  3, 19],\n",
       "        [10,  4, 13, 12, 21,  4, 14,  2,  9, 21,  7,  9,  4, 19],\n",
       "        [ 8,  8, 13,  8, 21,  7, 14,  2,  5, 21, 12, 10,  3, 19],\n",
       "        [ 4,  7, 13, 10, 21,  9, 14,  2,  4, 21,  3,  9,  7, 19],\n",
       "        [ 7,  5, 13,  8, 21, 12, 14,  2,  5, 21,  7, 10, 11, 19],\n",
       "        [ 8,  8, 13,  4, 21,  6, 14,  2, 10, 21,  4,  8, 19,  1],\n",
       "        [12, 12, 13,  5, 21, 12, 14,  2,  5, 21, 11, 10,  4, 19],\n",
       "        [ 5,  5, 13,  4, 21, 10, 14,  2,  6, 21, 10,  7, 19,  1],\n",
       "        [10,  4, 13,  9, 21,  9, 14,  2,  7, 21,  9, 11,  9, 19],\n",
       "        [ 4,  4, 13,  9, 21,  7, 14,  2, 10, 21,  3,  7, 19,  1],\n",
       "        [10,  4, 13,  6, 21, 12, 14,  2,  5, 21, 10,  9, 12, 19],\n",
       "        [ 5, 10, 13,  8, 21,  5, 14,  2,  4, 21,  7,  3,  7, 19],\n",
       "        [ 7,  4, 13,  8, 21, 11, 14,  2,  5, 21,  6, 10, 11, 19]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfe42d",
   "metadata": {},
   "source": [
    "$\\textbf{Issue 1}$. When including abstraction, the generated response don't 'stop' anymore. \n",
    "\n",
    "$\\textbf{Issue 2}$. Abstraction generation in 'train-time' mismatch with that of 'inference-time' (former is parallel search over query, latter is causal generation over answer).\n",
    "\n",
    "$\\textbf{Idea 1}$. We have mismatch between 'train-time' abstraction addition (which is on query), and 'test-time' abstraction addition (which is on answer). It's probably better to add abstraction on query, or prefix token sequence only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb46cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
