{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f514e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication Dataset (cot included/not, reverse/not)\n",
    "# - (no cot, no reverse) 11 x 12 = <answer> 132 \n",
    "# - (cot, no reverse) 11 x 12 = 12 + 120 = <answer> 132\n",
    "# - (no cot, reverse) 11 x 21 = <answer> 231 \n",
    "# - (cot, reverse) 11 x 21 = 21 + 021 = <answer> 231 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7693f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n",
      "Model initialized on cpu with 28.34M parameters.\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from model.model_minimind import MiniMindConfig\n",
    "from model.model_sorl import SorlModelWrapper\n",
    "from dataset.base import MemLoader\n",
    "from src.sorl import SORLConfig\n",
    "import os \n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration (Mimicking command-line args)\n",
    "# ==============================================================================\n",
    "args = SimpleNamespace(\n",
    "    # --- Paths ---\n",
    "    train_data_path=\"dataset/multiply/multiply_2x2_train_cot.bin\",\n",
    "    val_data_path=\"dataset/multiply/multiply_2x2_val_cot.bin\",\n",
    "    \n",
    "    # --- Model Config ---\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=2,\n",
    "    abstract_vocab_sizes=\"8\",\n",
    "    \n",
    "    # --- Training Config ---\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-4,\n",
    "    epoch=3,\n",
    "    \n",
    "    # --- SORL Config ---\n",
    "    n_rollout=5,\n",
    "    temperature=1.0,\n",
    "    K=4,\n",
    "    denoise_steps=1,\n",
    "    max_t_search=0,\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=False,\n",
    "    use_special_placeholders=False,\n",
    "    special_token_id=31,\n",
    "    abstract_budget=5,\n",
    "    temperature_flip=False,\n",
    "    \n",
    "    # --- Curriculum and Memory ---\n",
    "    curriculum_ratio=0.6, # looks redundant as of now, it (vaguely) violates the \"compositionality\" principle\n",
    "    use_fade_memory=False,\n",
    "    use_compression_mask=False, # <-- Set to True to test your new mask\n",
    "    compression_curriculum_ratio=0.25,\n",
    "    memory_span=128,\n",
    "    \n",
    "    # --- GAPT ---\n",
    "    default_phase=None, # Set to 1 or 2 to override, None to enable GAPT\n",
    "    delta=0.01,\n",
    "    tau=0.1,\n",
    "    p_m=10,\n",
    "    p_c=10\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "val_loader = MemLoader(args.val_data_path, device=args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_loader.tokenizer_path) # data is tokenized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = len(tokenizer)\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "# 2 layer 4 head\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = SorlModelWrapper.from_scratch(\n",
    "    config=minimind_config,\n",
    "    full_vocab_size_list=full_vocab_list,\n",
    "    memory_span=args.memory_span,\n",
    "    pad_token_id=pad_token_id\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model initialized on {args.device} with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")\n",
    "\n",
    "# --- SORL Config and Schedulers ---\n",
    "\n",
    "sorl_config = SORLConfig(\n",
    "    n=args.n_rollout, \n",
    "    temperature=args.temperature, \n",
    "    K=args.K,\n",
    "    l=1, \n",
    "    steps=args.denoise_steps, \n",
    "    max_t_search=args.max_t_search,\n",
    "    use_rhythmic_placeholders=args.use_rhythmic_placeholders,\n",
    "    use_spike_placeholders=args.use_spike_placeholders,\n",
    "    use_special_placeholders=args.use_special_placeholders,\n",
    "    special_token_id=args.special_token_id,\n",
    "    abstract_budget=args.abstract_budget,\n",
    "    temperature_flip=args.temperature_flip,\n",
    "    curriculum_ratio=args.curriculum_ratio,\n",
    "    use_fade_memory=args.use_fade_memory,\n",
    "    use_compression_mask=args.use_compression_mask,\n",
    "    min_keep=args.memory_span, \n",
    "    max_seq_len=train_loader.max_length,\n",
    "    train_iterations=int(args.epoch * len(train_loader) / args.batch_size), \n",
    "    train_batch_size=args.batch_size,\n",
    "    val_batch_size=args.batch_size,\n",
    "    max_length=train_loader.max_length,\n",
    "    default_phase=args.default_phase, \n",
    "    delta=args.delta, tau=args.tau,\n",
    "    p_m=args.p_m, p_c=args.p_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dce59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- No abstraction allowed\n",
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 3.51 (SSL: 3.510, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -41.0% | Abs-Free-Loss: 1.990 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 02 | Loss: 2.86 (SSL: 2.864, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -41.1% | Abs-Free-Loss: 1.931 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 03 | Loss: 2.77 (SSL: 2.766, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -41.1% | Abs-Free-Loss: 1.850 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 04 | Loss: 2.60 (SSL: 2.595, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -40.8% | Abs-Free-Loss: 1.770 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 05 | Loss: 2.50 (SSL: 2.497, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -43.0% | Abs-Free-Loss: 1.675 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 06 | Loss: 2.42 (SSL: 2.422, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -39.9% | Abs-Free-Loss: 1.675 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 07 | Loss: 2.32 (SSL: 2.322, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -42.0% | Abs-Free-Loss: 1.582 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 08 | Loss: 2.24 (SSL: 2.244, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -42.9% | Abs-Free-Loss: 1.506 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 09 | Loss: 2.15 (SSL: 2.151, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -39.3% | Abs-Free-Loss: 1.493 | t_search: 0 | drop_ratio: 0.00\n",
      "Step 10 | Loss: 2.09 (SSL: 2.095, Abs: 0.00) | Advantage: 0.0% | Info-Gain: -40.8% | Abs-Free-Loss: 1.435 | t_search: 0 | drop_ratio: 0.00\n"
     ]
    }
   ],
   "source": [
    "from src.sorl import evaluate \n",
    "from src.sorl import compute_per_token_loss, compute_loss, sorl_search, SearchScheduler, GatedPhaseTransition\n",
    "\n",
    "# First, test out baseline performance, then test out SoRL performance etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "search_scheduler = SearchScheduler(sorl_config)\n",
    "gapt = GatedPhaseTransition(sorl_config.delta, sorl_config.tau, sorl_config.p_m, sorl_config.p_c)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "\n",
    "\n",
    "# for i in range(sorl_config.train_iterations): # Run for 10 steps\n",
    "for i in range(10):\n",
    "    # --- Scheduler Step ---\n",
    "    t_search, drop_ratio = search_scheduler.step()\n",
    "    sorl_config.max_t_search = 0\n",
    "    model.drop_ratio = 0.0\n",
    "\n",
    "    # --- Get data and perform SORL search ---\n",
    "    # (1). Apply loss mask (and change its shape with abs padding) || (2). Customize abs padding\n",
    "    data, loss_mask = train_loader.get_batch(sorl_config.train_batch_size)\n",
    "    with torch.no_grad():\n",
    "        search_data, switch_ratio = sorl_search(data, loss_mask, model, sorl_config)\n",
    "        \n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, search_data)\n",
    "    ssl_loss, abs_loss = compute_loss(search_data, model, ppt, loss_mask)\n",
    "    \n",
    "    total_loss = ssl_loss + abs_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    greedy_advantage, best_advantage, greedy_info_gain, _, a_loss = evaluate(data, loss_mask, sorl_config, model, search_n=1)\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.2f} (SSL: {ssl_loss.item():.3f}, Abs: {abs_loss.item():.2f}) | \"\n",
    "        f\"Advantage: {greedy_advantage:.1f}% | Info-Gain: {greedy_info_gain:.1f}% | Abs-Free-Loss: {a_loss:.3f} | \"\n",
    "        f\"t_search: {t_search} | \"\n",
    "        f\"drop_ratio: {model.drop_ratio:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1f7f",
   "metadata": {},
   "source": [
    "$\\textbf{Question} 1$. What'd happen if the topological similarity approaches 1, and how can we make it so?\n",
    "$\\textbf{Question} 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b99adf",
   "metadata": {},
   "source": [
    "$\\textbf{Observation 1}$. 200 epochs is far from enough for minimid model to learn 2x2 multiplication. Think 2k epochs at least. We could also use a bigger batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0131bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Batches: 100%|██████████| 20/20 [00:10<00:00,  1.87it/s, Accuracy=0.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Summary ---\n",
      "Samples Evaluated: 200\n",
      "Correct Predictions: 0\n",
      "Accuracy: 0.00%\n",
      "Topological Similarity: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'correct': 0,\n",
       " 'total': 200,\n",
       " 'top_sim_score': 0.21545161535022445}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_multiply import evaluate_on_loader\n",
    "\n",
    "# --- Evaluation (Generate & Check Answer) ---- \n",
    "print(\"--- Running Evaluation ---\")\n",
    "evaluate_on_loader(model, tokenizer, val_loader, batch_size=10, K=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f285986",
   "metadata": {},
   "source": [
    "$\\textbf{Record 1.}$.  1.5 epochs (that's 80k * 1.5 = 120k data getting trained, with batch size of 128, roughly 1k iterations required) on 2x2 multiplication produces 66.5% accuracy on test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb81ac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f497bd9d8c864343939c2cb8f8d3cc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca4191d80854b28bd67456b558d47d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load the pretrained \"gpt2\" model (which is the 124M parameter version)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# You can print the model architecture to see its layers\n",
    "# print(model)\n",
    "\n",
    "# To get the total number of parameters, you can do the following:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# And to see how many of those are trainable:\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f2499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b4952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dbc2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 1 2 * 4 8 =\n",
      "Generated Response: <answer> 5 5 6 <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "Expected Answer:  5 7 6\n",
      "Generated Answer: 5 5 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('5 5 6', '5 7 6')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval_multiply import evaluate_multiplication\n",
    "\n",
    "input_ids, loss_mask = val_loader.get_batch(1)\n",
    "prompt = tokenizer.decode(input_ids[0])\n",
    "evaluate_multiplication(model, tokenizer, prompt, K=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a518ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate train_loader data is correct \n",
    "from eval_multiply import _get_query_and_gt_ids\n",
    "\n",
    "input_ids, loss_mask = train_loader.get_batch(1)\n",
    "prompt = tokenizer.decode(input_ids[0])\n",
    "\n",
    "query_str = prompt.split('=')[0].strip() # execute this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38792e8",
   "metadata": {},
   "source": [
    "$\\textbf{Idea 3}$. How about using RL instead? \n",
    "\n",
    "$\\textbf{Idea 4}$. How about using SoRL instead? \n",
    "\n",
    "$\\textbf{Idea 5}$. How about using SoRL + RL instead? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8431a",
   "metadata": {},
   "source": [
    "Issue 1. Missing CoT generations. \n",
    "- $\\textit{Fix 1}$. Dynamically identify case requiring CoT and case that doesn't. \n",
    "\n",
    "Issue 2. I can't believe transformer can't learn multiplication (2 digits !?) -- Can we initialize a Qwen architecture and try on this again? Can we scale up the experiment script and run on GPU instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74b00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c6075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62cfe42d",
   "metadata": {},
   "source": [
    "$\\textbf{Issue 1}$. When including abstraction, the generated response don't 'stop' anymore. \n",
    "\n",
    "$\\textbf{Issue 2}$. Abstraction generation in 'train-time' mismatch with that of 'inference-time' (former is parallel search over query, latter is causal generation over answer).\n",
    "\n",
    "$\\textbf{Idea 1}$. We have mismatch between 'train-time' abstraction addition (which is on query), and 'test-time' abstraction addition (which is on answer). It's probably better to add abstraction on query, or prefix token sequence only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb46cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing components ---\n"
     ]
    }
   ],
   "source": [
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Initialization\n",
    "# ==============================================================================\n",
    "print(\"--- Initializing components ---\")\n",
    "# --- Tokenizer and Data ---\n",
    "train_loader = MemLoader(args.train_data_path, device=args.device)\n",
    "val_loader = MemLoader(args.val_data_path, device=args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_loader.tokenizer_path) # data is tokenized\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# --- Model ---\n",
    "base_vocab_size = len(tokenizer)\n",
    "abstract_vocab_sizes = [int(v) for v in args.abstract_vocab_sizes.split(',')]\n",
    "full_vocab_list = [base_vocab_size] + abstract_vocab_sizes\n",
    "\n",
    "# 2 layer 4 head\n",
    "minimind_config = MiniMindConfig(\n",
    "    hidden_size=args.hidden_size,\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_hidden_layers,\n",
    "    vocab_size=sum(full_vocab_list)\n",
    ")\n",
    "\n",
    "model = MiniMindForCausalLM(minimind_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1616a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting interactive training loop ---\n",
      "Step 01 | Loss: 1.24\n",
      "Step 02 | Loss: 1.39\n",
      "Step 03 | Loss: 1.21\n",
      "Step 04 | Loss: 1.34\n",
      "Step 05 | Loss: 1.36\n",
      "Step 06 | Loss: 1.30\n",
      "Step 07 | Loss: 1.32\n",
      "Step 08 | Loss: 1.23\n",
      "Step 09 | Loss: 1.22\n",
      "Step 10 | Loss: 1.26\n",
      "Step 11 | Loss: 1.28\n",
      "Step 12 | Loss: 1.20\n",
      "Step 13 | Loss: 1.22\n",
      "Step 14 | Loss: 1.17\n",
      "Step 15 | Loss: 1.18\n",
      "Step 16 | Loss: 1.23\n",
      "Step 17 | Loss: 1.21\n",
      "Step 18 | Loss: 1.16\n",
      "Step 19 | Loss: 1.26\n",
      "Step 20 | Loss: 1.13\n",
      "Step 21 | Loss: 1.13\n",
      "Step 22 | Loss: 1.24\n",
      "Step 23 | Loss: 1.12\n",
      "Step 24 | Loss: 1.23\n",
      "Step 25 | Loss: 1.11\n",
      "Step 26 | Loss: 1.11\n",
      "Step 27 | Loss: 1.17\n",
      "Step 28 | Loss: 1.21\n",
      "Step 29 | Loss: 1.19\n",
      "Step 30 | Loss: 1.20\n",
      "Step 31 | Loss: 1.26\n",
      "Step 32 | Loss: 1.18\n",
      "Step 33 | Loss: 1.15\n",
      "Step 34 | Loss: 1.09\n",
      "Step 35 | Loss: 1.14\n",
      "Step 36 | Loss: 1.16\n",
      "Step 37 | Loss: 1.05\n",
      "Step 38 | Loss: 1.12\n",
      "Step 39 | Loss: 1.20\n",
      "Step 40 | Loss: 1.06\n",
      "Step 41 | Loss: 1.18\n",
      "Step 42 | Loss: 1.14\n",
      "Step 43 | Loss: 1.11\n",
      "Step 44 | Loss: 1.17\n",
      "Step 45 | Loss: 1.19\n",
      "Step 46 | Loss: 1.11\n",
      "Step 47 | Loss: 1.05\n",
      "Step 48 | Loss: 1.14\n",
      "Step 49 | Loss: 1.19\n",
      "Step 50 | Loss: 1.17\n",
      "Step 51 | Loss: 1.12\n",
      "Step 52 | Loss: 1.12\n",
      "Step 53 | Loss: 1.16\n",
      "Step 54 | Loss: 1.19\n",
      "Step 55 | Loss: 1.17\n",
      "Step 56 | Loss: 1.18\n",
      "Step 57 | Loss: 1.08\n",
      "Step 58 | Loss: 1.14\n",
      "Step 59 | Loss: 1.08\n",
      "Step 60 | Loss: 1.19\n",
      "Step 61 | Loss: 1.09\n",
      "Step 62 | Loss: 1.08\n",
      "Step 63 | Loss: 1.01\n",
      "Step 64 | Loss: 1.16\n",
      "Step 65 | Loss: 1.11\n",
      "Step 66 | Loss: 1.09\n",
      "Step 67 | Loss: 1.11\n",
      "Step 68 | Loss: 1.09\n",
      "Step 69 | Loss: 1.08\n",
      "Step 70 | Loss: 1.07\n",
      "Step 71 | Loss: 1.13\n",
      "Step 72 | Loss: 1.07\n",
      "Step 73 | Loss: 1.09\n",
      "Step 74 | Loss: 1.12\n",
      "Step 75 | Loss: 0.98\n",
      "Step 76 | Loss: 1.04\n",
      "Step 77 | Loss: 1.06\n",
      "Step 78 | Loss: 1.08\n",
      "Step 79 | Loss: 1.12\n",
      "Step 80 | Loss: 1.06\n",
      "Step 81 | Loss: 1.11\n",
      "Step 82 | Loss: 1.12\n",
      "Step 83 | Loss: 1.10\n",
      "Step 84 | Loss: 1.03\n",
      "Step 85 | Loss: 1.09\n",
      "Step 86 | Loss: 1.11\n",
      "Step 87 | Loss: 1.11\n",
      "Step 88 | Loss: 1.15\n",
      "Step 89 | Loss: 1.11\n",
      "Step 90 | Loss: 1.09\n",
      "Step 91 | Loss: 1.15\n",
      "Step 92 | Loss: 1.12\n",
      "Step 93 | Loss: 1.08\n",
      "Step 94 | Loss: 1.19\n",
      "Step 95 | Loss: 0.99\n",
      "Step 96 | Loss: 1.09\n",
      "Step 97 | Loss: 1.11\n",
      "Step 98 | Loss: 1.17\n",
      "Step 99 | Loss: 1.16\n",
      "Step 100 | Loss: 1.08\n",
      "Step 101 | Loss: 1.11\n",
      "Step 102 | Loss: 1.21\n",
      "Step 103 | Loss: 1.11\n",
      "Step 104 | Loss: 1.00\n",
      "Step 105 | Loss: 1.09\n",
      "Step 106 | Loss: 1.09\n",
      "Step 107 | Loss: 1.03\n",
      "Step 108 | Loss: 1.04\n",
      "Step 109 | Loss: 1.16\n",
      "Step 110 | Loss: 1.02\n",
      "Step 111 | Loss: 1.04\n",
      "Step 112 | Loss: 1.09\n",
      "Step 113 | Loss: 1.13\n",
      "Step 114 | Loss: 1.15\n",
      "Step 115 | Loss: 1.03\n",
      "Step 116 | Loss: 1.12\n",
      "Step 117 | Loss: 1.07\n",
      "Step 118 | Loss: 1.07\n",
      "Step 119 | Loss: 1.05\n",
      "Step 120 | Loss: 1.02\n",
      "Step 121 | Loss: 0.97\n",
      "Step 122 | Loss: 1.04\n",
      "Step 123 | Loss: 1.09\n",
      "Step 124 | Loss: 1.09\n",
      "Step 125 | Loss: 1.09\n",
      "Step 126 | Loss: 1.04\n",
      "Step 127 | Loss: 1.06\n",
      "Step 128 | Loss: 1.04\n",
      "Step 129 | Loss: 1.08\n",
      "Step 130 | Loss: 1.07\n",
      "Step 131 | Loss: 1.06\n",
      "Step 132 | Loss: 0.99\n",
      "Step 133 | Loss: 1.05\n",
      "Step 134 | Loss: 1.10\n",
      "Step 135 | Loss: 1.03\n",
      "Step 136 | Loss: 1.01\n",
      "Step 137 | Loss: 1.00\n",
      "Step 138 | Loss: 0.96\n",
      "Step 139 | Loss: 0.99\n",
      "Step 140 | Loss: 1.04\n",
      "Step 141 | Loss: 1.01\n",
      "Step 142 | Loss: 1.01\n",
      "Step 143 | Loss: 1.06\n",
      "Step 144 | Loss: 1.08\n",
      "Step 145 | Loss: 0.99\n",
      "Step 146 | Loss: 0.99\n",
      "Step 147 | Loss: 1.06\n",
      "Step 148 | Loss: 1.07\n",
      "Step 149 | Loss: 1.03\n",
      "Step 150 | Loss: 0.99\n",
      "Step 151 | Loss: 1.06\n",
      "Step 152 | Loss: 1.08\n",
      "Step 153 | Loss: 0.96\n",
      "Step 154 | Loss: 1.03\n",
      "Step 155 | Loss: 1.00\n",
      "Step 156 | Loss: 1.05\n",
      "Step 157 | Loss: 0.97\n",
      "Step 158 | Loss: 0.96\n",
      "Step 159 | Loss: 1.04\n",
      "Step 160 | Loss: 1.00\n",
      "Step 161 | Loss: 1.09\n",
      "Step 162 | Loss: 1.02\n",
      "Step 163 | Loss: 1.00\n",
      "Step 164 | Loss: 0.98\n",
      "Step 165 | Loss: 0.96\n",
      "Step 166 | Loss: 1.04\n",
      "Step 167 | Loss: 1.02\n",
      "Step 168 | Loss: 1.02\n",
      "Step 169 | Loss: 1.02\n",
      "Step 170 | Loss: 0.97\n",
      "Step 171 | Loss: 1.02\n",
      "Step 172 | Loss: 1.12\n",
      "Step 173 | Loss: 1.08\n",
      "Step 174 | Loss: 1.02\n",
      "Step 175 | Loss: 1.02\n",
      "Step 176 | Loss: 1.07\n",
      "Step 177 | Loss: 1.07\n",
      "Step 178 | Loss: 0.96\n",
      "Step 179 | Loss: 1.11\n",
      "Step 180 | Loss: 1.02\n",
      "Step 181 | Loss: 0.97\n",
      "Step 182 | Loss: 1.02\n",
      "Step 183 | Loss: 1.04\n",
      "Step 184 | Loss: 1.10\n",
      "Step 185 | Loss: 1.05\n",
      "Step 186 | Loss: 1.09\n",
      "Step 187 | Loss: 1.01\n",
      "Step 188 | Loss: 1.06\n",
      "Step 189 | Loss: 0.97\n",
      "Step 190 | Loss: 1.06\n",
      "Step 191 | Loss: 1.07\n",
      "Step 192 | Loss: 1.04\n",
      "Step 193 | Loss: 0.95\n",
      "Step 194 | Loss: 1.08\n",
      "Step 195 | Loss: 1.04\n",
      "Step 196 | Loss: 1.02\n",
      "Step 197 | Loss: 1.06\n",
      "Step 198 | Loss: 1.05\n",
      "Step 199 | Loss: 1.12\n",
      "Step 200 | Loss: 1.08\n"
     ]
    }
   ],
   "source": [
    "from src.sorl import evaluate \n",
    "from src.sorl import compute_per_token_loss, compute_loss, sorl_search, SearchScheduler, GatedPhaseTransition\n",
    "\n",
    "# First, test out baseline performance, then test out SoRL performance etc.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Interactive Training Loop\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Starting interactive training loop ---\")\n",
    "model.train()\n",
    "\n",
    "for i in range(sorl_config.train_iterations): # Run for 10 steps\n",
    "\n",
    "    # --- Get data and perform SORL search ---\n",
    "    # (1). Apply loss mask (and change its shape with abs padding) || (2). Customize abs padding\n",
    "    data, loss_mask = train_loader.get_batch(sorl_config.train_batch_size)\n",
    "    # break\n",
    "    # --- Compute loss ---\n",
    "    ppt = compute_per_token_loss(model, data, tokenizer.pad_token_id)\n",
    "    ssl_loss = (ppt * loss_mask[:, 1:]).sum() / loss_mask[:, 1:].sum()\n",
    "    total_loss = ssl_loss\n",
    "    \n",
    "    # --- Optimizer step ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Step {i+1:02d} | \"\n",
    "        f\"Loss: {total_loss.item():.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d044737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
